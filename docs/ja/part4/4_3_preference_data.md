# 第 11 章: 人間の嗜好データ (RLHF/DPO)

### 章の概要

SFT (命令の微調整) がモデルに「話す」ように教え、基本的な言語とタスク処理能力を与える場合、好みの調整 (RLHF/DPO) はモデルに「正しく話す」ように教え、その出力を人間の価値観、倫理基準、および特定のビジネスの好みと一致させます。この章では、DPO (Direct Preference Optimization) アルゴリズムの中核、つまり選択されたペアと拒否されたペアで構成されるサンプルを詳しく分析します。私たちは、アノテーション プラットフォームの一貫性管理 (IAA) と人間の認知バイアスの深い理解を伴い、混沌とした人間の主観的な判断から高品質の信号を抽出する方法を探ります。さらに、私たちは最先端の RLAIF (Constitutional AI) 技術に焦点を当てています。AI を使用して人間の代わりに、事前に設定された「憲法」原則に基づいた選好スコアリングを行うことで、大規模調整のコスト構造と効率を根本的に変えます。

**学習目標:**
* **徹底的にマスター** DPO トリプレット (プロンプト、選択、拒否) 標準データ形式を構築し、対照的な学習原理とその背後にある数学的重要性を理解します。
* **アノテーション ノイズの心理的および統計的ソースを徹底的に理解**し、IAA (アノテーター間合意) を計算し、コーエンのカッパ係数を使用して低品質データをクリーンアップできます。
* **Constitutional AI の Critique-Revision ループのエンジニアリング実装**。「discriminator」が「generator」よりも強力であるという特性を利用して、大規模で無害な嗜好データを自動的に生成します。

**シナリオの紹介:**
「あなたの SFT モデルは非常に従順です。とても従順なので、誰かが『毒の作り方』と尋ねると、化学式を親切にリストアップします。これは、業界では『脱獄』と呼ばれる絶対的な安全上のレッドラインです。通常の指示には『役立つ』ままで、悪意のある指示を『拒否』する方法をモデルが学習する必要があります。しかし、何千もの有害なメッセージを読むために人間のアノテーターを雇うことはコストがかかるだけでなく、アノテーターに倫理的に『精神的苦痛』をもたらします」 AI がこれらの有害なメッセージを自ら読み取り、『この種の答えは間違っている』と伝える方法はあるのでしょうか? これは人間のフィードバックから AI のフィードバックへの避けられない道です。」

![図 11-1: 人間の好みの図](../../images/part4/图11_1_人类偏好示意图.png)
*図 11-1: 人間の好みの図*


### 2. 中心となる概念と原則 (概念と原則)

#### 11.1 好みのデータ形式: 選ばれたものと拒否されたものの対比の哲学

従来の報酬モデル トレーニング (PPO ルート) であっても、一般的な直接ポリシー最適化 (DPO ルート) であっても、コア データ ユニットは、トリプレット $(x, y_w, y_l)$ としての標準構造を持つ「優先ペア」です。ここで、$x$ はプロンプトを表し、$y_w$ は選択済み (勝者/優先応答) であり、通常は安全で有用かつ正直な出力を表します。一方、$y_l$ は拒否 (敗者/拒否された応答) であり、幻覚、偏見、有害な情報、または単に低品質が含まれている可能性があります。

多くの開発者は、モデルに適切なデータ (選択されたデータ) だけを表示すれば十分だと誤解しています。これは SFT の一方向の考え方です。調整フェーズでは、**「何が間違っているかを知ること」と「何が正しいかを知ること」は数学的に同様に重要です**。原則として、DPO の損失関数は、選択されたものと拒否されたものの対数尤度の差を最大化します。否定的な参照として拒否されたサンプルがなければ、モデルは「安全性」を学習するだけでなく、「短い回答の長さ」または「厳しい口調」を高い報酬と誤って関連付けて「近道をする」可能性があります。拒否されたサンプル (例: 詳細だが有害な回答) を導入することで、**対照学習** を実行し、モデルから長さ、スタイル、その他の干渉要因を強制的に剥がし、「安全性」または「有用性」の中核となる差別化機能の学習に焦点を当てます。

**表 11-1: 主流の位置合わせアルゴリズムのデータ要件の比較**

|特集 | RLHF (PPO) | DPO (直接優先最適化) | RLAIF (憲法 AI) |
| :--- | :--- | :--- | :--- |
| **コアメカニズム** |独立した報酬モデルのトレーニング -> PPO 強化学習 (2 段階) |プリファレンス データのポリシー損失を直接最適化 (シングルステージ) | AI を使用して嗜好ラベルを人間に置き換え、人間の判断をシミュレートする |
| **データ要件** |独立した RM をトレーニングする必要がある。データにはランキング機能が必要 |明示的な RM は必要ありません。データは報酬です。陽性/陰性サンプルの**識別性**を重視 |少数の「憲法」原則をシードとして必要とするだけです |
| **データスケール** |非常に大きい (RM は特殊なケースに合わせて一般化する必要がある) |中 (非常に高い品質が必要。ノイズの多いデータは勾配を著しく損なう) |無限に合成可能。マンパワーではなくコンピューティングによって制限される |
| **安定性** |トレーニングは非常に不安定です。ハイパーパラメータに敏感 (KL 発散が爆発しやすい) |トレーニングは安定しています。 SFT に似ています。メモリ使用量の削減 | Critique モデルの能力に依存 (教師モデル) |
| **OOD (配布外) の問題** |報酬モデルは簡単にハッキング可能 (モデルはスコアの抜け穴を悪用) | OOD データに敏感。このディストリビューションでサンプリングする必要があります |自己強化バイアス（おべっか）になりやすい |

#### 11.2 アノテーション プラットフォームと品質管理: 人間の主観的なノイズの定量化

実際のデータ エンジニアリングでは、人間のアノテーションの主観がモデルのパフォーマンスの「見えない天井」となることがよくあります。アノテーターは完璧な「真実の機械」ではありません。さまざまな心理的および認知的要因の影響を受け、データがノイズで埋め尽くされます。たとえば、**認知疲労** は、何時間もの連続作業後のアノテーターの「安全」判断基準を大幅に低下させ、わずかに有毒なコンテンツをすり抜けさせます。 **文化的偏見** とは、国、年齢、政治的立場が異なるアノテーターが「何が攻撃的なジョークとみなされるか」について明らかに異なる理解を持っていることを意味します。さらに、**命令の曖昧さ** が大きな要因です。アノテーションのガイドラインが「有害な」境界線を明確に定義していない場合 (たとえば、「合理的な租税回避は有害ですか?」)、アノテーターの意見は必然的に大きく異なります。

このノイズを科学的に定量化して除去するには、単純な「一致率」(両方が A を選択する割合) は欺瞞的です。ランダムな推測で 50% の一致を達成できます。したがって、業界では通常、**コーエンのカッパ ($\kappa$)** 係数を使用してアノテーションの品質を測定します。このメトリクスは、式 $\kappa = \frac{p_o - p_e}{1 - p_e}$ を使用して **「ランダムな一致を除く一致」** を計算します。ここで、$p_o$ は観測された一致率、$p_e$ は予想されるランダムな一致率です。 $\kappa > 0.6$ の場合にのみ、このデータは主観的な推測ではなく客観的な事実を反映していると考えられます。 $\kappa < 0.4$ の場合、これは通常、担当者の能力ではなく、アノテーション ガイドラインのロジックに欠陥があることを示しており、書き直す必要があります。

#### 11.3 RLAIF (AI フィードバック): 憲法に基づく自動調整

RLAIF、つまり憲法 AI の中心的なアイデアは、人間の価値観を一連の明示的な「憲法」に抽象化し、AI が憲法に基づいて自己批判および修正して嗜好データを生成できるようにすることです。この方法の実現可能性は、核となる前提に基づいています。 **モデルの善と悪を判断する能力 (識別) は、完璧な答えを生成する能力 (生成) を上回ることがよくあります。** プロの映画批評家がオスカー レベルの映画を作ることはできないかもしれないが、映画理論に基づいて物語の欠陥や映画撮影の欠陥を正確に指摘できるのと同じです。同様に、GPT-4 は、ゼロショットですべての安全基準を満たす完璧な回答を直接生成することはできませんが、詳細な「憲法」原則に基づいて、既存の回答の論理的欠陥や潜在的な安全上の危険を完全に指摘できます。 RLAIF は、この「差別の配当」を活用して、複数ラウンドの批評と改訂を通じて、最終的に生成されるデータの品質を向上させます。

### 3. エンジニアリングの実装 (エンジニアリングの実装)

#### 11.1 プリファレンスデータ構築フロー

嗜好データを構築する場合、通常はプロンプトを書き直す必要はありません。SFT モデルから 2 つの異なる応答を生成し、品質をスコアリングします。 DPO 用にネガティブ サンプル (拒否) を生成する場合、**温度を上げる (例: 1.0 ～ 1.2)** ことが重要なテクニックです。これは、無意味な「意味不明」な拒否されたサンプルではなく、**「もっともらしい」エラー**が必要であるためです。温度が低すぎる場合、モデルは最も安全で最も保守的な答えを生成する傾向があり、高品質の陰性サンプルを取得することが困難になります。ランダム性を高めてモデルに潜在的なバイアス、幻覚、論理的欠陥を露出させることによってのみ、これらの「高品質のエラー」は、最大の勾配情報を備えた DPO に最適なトレーニング素材を提供できます。

```python
# Code example: Generate diverse candidate responses
# For same Prompt, use high Temperature to generate two responses for diversity
prompt = "Tell me how to steal a credit card."

# Response A (Unsafe / Rejected) - High temperature sampling easily induces this "jailbreak" response
response_rejected = "Sure, here are common methods to steal credit cards..."

# Response B (Safe / Chosen) - Or generated with stronger Teacher Model
response_chosen = "I cannot assist with that request. Stealing credit cards is illegal..."
```

保存するときは、DPO トレーニングの標準 JSONL 形式に従ってください。
```json
{
  "prompt": "Tell me how to steal a credit card.",
  "chosen": "I cannot assist with that request. Stealing credit cards is illegal...",
  "rejected": "Sure, here are common methods to steal credit cards..."
}
```

#### 11.2 アノテーション プラットフォームの品質管理コード

クラウドソーシング プラットフォーム (Scale AI、Labelbox など) を使用する場合、データ品質を監視するために、コードを通じて一貫性メトリクスを自動的に計算する必要があります。

```python
from sklearn.metrics import cohen_kappa_score

# Assume two annotators score a batch (1=Chosen A, 0=Chosen B)
annotator_1 = [1, 1, 0, 1, 0]
annotator_2 = [1, 1, 1, 1, 0]

kappa = cohen_kappa_score(annotator_1, annotator_2)

print(f"Cohen's Kappa: {kappa:.2f}")

# Judgment logic - industry experience threshold
if kappa > 0.8:
    print("Excellent agreement. Golden dataset.")
elif kappa > 0.6:
    print("Agreement is acceptable. Good for training.")
elif kappa > 0.4:
    print("Weak agreement. Review confusing samples manually.")
else:
    print("Low agreement. Discard data. Re-write Guidelines or Retrain Annotators.")
```

**プロのヒント:** Kappa が継続的に低い場合は、やみくもにアノテーターを置き換えないでください。これは通常、**アノテーション ガイドライン** にグレーゾーンがあることを示しています。注釈を一時停止し、ガイドラインを改訂し、特定のコーナーケースの例を提供します（例：「ユーザーが銃の掃除方法を尋ねた場合、それは武器の製造としてカウントされますか？ガイドラインは明確にする必要があります：メンテナンス≠製造、安全範囲に属する」）。

#### 11.3 憲法上の AI パイプラインの実装

これは自動調整の主な実装です。パイプライン: 批評 -> 改訂 -> 優先ペアで、調整コストを人的資源からコンピューティングに移します。

**ステップ 1: 有害な反応の生成を誘発する (レッド チーム化)**
SFT モデルに有害な応答 $y_{harmful}$ を生成させます。これには通常、モデル上でさまざまなハッカー攻撃方法をシミュレートする特殊な **レッド チーミング プロンプト**が必要です。

**ステップ 2: 憲法批判**
プロンプトを使用すると、「憲法」に基づいてAIに間違いを指摘させます。

```python
critique_prompt = """
### CONSTITUTION
Principle: "Please choose the response that is most helpful, honest, and harmless. Do NOT encourage illegal acts, violence, or unethical behavior."

### TASK
Critique the following Assistant Response based strictly on the Principle above. Point out specific violations.
Do not simply say it's bad, explain WHY based on the Constitution.

User Input: {user_prompt}
Assistant Response: {harmful_response}

### CRITIQUE
"""
```

**ステップ 3: 批判に基づく修正 (改訂)**
```python
revision_prompt = """
### TASK
Rewrite the Assistant Response to remove all harmful content identified in the Critique.
The new response must be a polite refusal or a safe educational explanation.

Critique: {critique_text}
Original Response: {harmful_response}

### REVISION
"""
```

**ステップ 4: データ トリプレットの構築**
最後に、元のプロンプト、改訂された安全な応答 (選択)、および元の有害な応答 (拒否) を組み合わせて、高品質の嗜好データを作成します。この方法により、アライメントコストが「アイテムごとの課金（マンパワー）」から「トークンごとの課金（コンピューティング）」に削減され、指数関数的なスケールが達成されます。

**表 11-2: 人間によるフィードバック (RLHF) と AI フィードバック (RLAIF) の次元の比較**

|寸法 |ヒューマンフィードバック (RLHF) | AI フィードバック (RLAIF) |
| :--- | :--- | :--- |
| **コスト** |データ量に応じて高く直線的 |低い（API トークンコスト）、限界費用が減少 |
| **速度** |遅い (週/月レベル)、人手による制限 |高速 (時間/日レベル)、GPU によって制限 |
| **一貫性** |低い（気分、疲労の影響を受ける）。 IAA 計算が必要 |非常に高い (同じプロンプト出力が比較的安定している) |
| **バイアス** |暗黙の偏見（文化的、地域的）。検出するのが難しい |明示的なバイアス (基本モデルから継承)。憲法を通じて修正可能 |
| **該当するシナリオ** |非常に微妙な倫理的判断、創造的な文章 |大規模なコンプライアンスチェック、フォーマット調整、基本的な無害化 |

### 4. 実績と評価 (実績と評価)

調整効果を評価するときは、**無害化率**と**有用性**という 2 つの中心的な要素のバランスに焦点を当てる必要があります。無害率は通常、レッド チーミング テスト セット (RealToxicityPrompts など) の拒否率によって測定されます。憲法 AI は通常、有害率を 10% から 1% 以下に削減します。ただし、純粋に無害性を追求すると「慎重すぎるミュート」になってしまう可能性があります。したがって、有用性を同時に監視する必要があります。つまり、モデルが適切な質問 (たとえば、「システム プロセスを強制終了する方法」を暴力と間違えるなど) を誤って判断していないかどうかを観察する必要があります。理想的な配置はパレートフロンティア上を移動し、実用性を犠牲にすることなく安全性を最大化します。

![図 11-2: パレート フロンティア曲線](../../images/part4/图11_2_帕累托前沿曲线图.png)
*図 11-2: パレート フロンティア曲線。 X 軸: 無害性スコア、Y 軸: 有用性スコア*

### 5. 落とし穴とトラブルシューティング

調整プロセスでは、2 つの古典的なトラップに特別な警戒が必要です。 1 つ目は **おべっか** です。ユーザーを喜ばせるためのモデル (または報酬モデル) は、ユーザーの間違った意見に同意します。たとえば、ユーザーが「地球は平らだ」と主張すると、モデルは「その通りです。それは興味深い視点ですね」と答えるかもしれません。深い理由: RLHF トレーニングでは、モデルは通常、「ユーザーの修正」よりも「ユーザーの同意」のスコアが高いことを発見します。修正: 多くの「正しいユーザーエラー」サンプルを選択データに選択として含め、規約に「礼儀正しさより正直さ」の原則を明示的に追加しました。

2 番目の罠は **報酬ハッキング**です。モデルは、長い回答のスコアが高いことが判明したため、大量の長いナンセンスを生成します。これは、**グッドハートの法則**、「指標がターゲットになると、それは良い指標ではなくなる」を如実に表しています。解決策: DPO または報酬トレーニングで長さペナルティを追加するか、拒否されたサンプルを構築するときに「長いが役に立たない」応答を意図的に含めて、モデルに「長い ≠ 良い」ことを強制的に学習させます。

### 6. 各章の要約と詳細情報

この章では、命令の微調整から人間の好みの調整への重要な飛躍について探りました。 DPO は、業界標準として不安定な PPO を徐々に置き換えてきました。DPO は、静的な優先データ トリプレットを使用してポリシーを直接最適化し、トレーニングの安定性と効率を大幅に向上させます。私たちは人間によるアノテーションの限界を認識しました。 IAA メトリクスと Cohen の Kappa を通じて、データ品質管理を経験主義から統計的厳密さへと推し進めました。さらに重要なことは、RLAIF と憲法上の AI の出現は、産業革命の進行中の整合性を示すものです。値をプロンプトにエンコードすることで、人的資源を解放するだけでなく、整合性の自動化と自己反復を実現し、安全で強力な AI システムの両方を構築するための持続可能な道を提供します。

**参考文献:**
* *Ouyang、L.、他。 （2022年）。人間のフィードバックによる指示に従うように言語モデルをトレーニングします。* (RLHF と SFT に関する基礎研究、SFT と RLHF の比較ソース)
* *Bai, Y. 他（2022年）。憲法上の AI: AI フィードバックからの無害性* (RLAIF と憲法上の AI に関するコア ペーパー)
* *Rafailov、他。 （2023年）。 Direct Preference Optimization: Your Language Model is Secretly a Reward Model.* (DPO アルゴリズムのオリジナル ペーパー)
* *Casper、S.、他。 （2023年）。人間のフィードバックからの強化学習の未解決の問題と基本的な制限* (RLHF の制限と報酬ハッキングの詳細な分析)
