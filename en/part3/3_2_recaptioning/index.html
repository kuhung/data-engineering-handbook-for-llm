<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="大模型数据工程：架构、算法及项目实战">
      
      
        <meta name="author" content="ustc">
      
      
        <link rel="canonical" href="https://datascale-ai.github.io/data_engineering_book/en/part3/3_2_recaptioning/">
      
      
        <link rel="prev" href="../3_1_image_text_pairs/">
      
      
        <link rel="next" href="../3_3_video_audio/">
      
      
        
          <link rel="alternate" href="../../../part3/3_2_recaptioning/" hreflang="zh">
        
          <link rel="alternate" href="./" hreflang="en">
        
          <link rel="alternate" href="../../../ja/part3/3_2_recaptioning/" hreflang="ja">
        
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>Chapter 7: Recaptioning - Data Engineering for Large Models: Architecture, Algorithms &amp; Projects</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-7-data-recaptioning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../" title="Data Engineering for Large Models: Architecture, Algorithms &amp; Projects" class="md-header__button md-logo" aria-label="Data Engineering for Large Models: Architecture, Algorithms &amp; Projects" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Data Engineering for Large Models: Architecture, Algorithms &amp; Projects
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chapter 7: Recaptioning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="red" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="red" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../../part3/3_2_recaptioning/" hreflang="zh" class="md-select__link">
              简体中文
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../../ja/part3/3_2_recaptioning/" hreflang="ja" class="md-select__link">
              日本語
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/datascale-ai/data_engineering_book/tree/main" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../" title="Data Engineering for Large Models: Architecture, Algorithms &amp; Projects" class="md-nav__button md-logo" aria-label="Data Engineering for Large Models: Architecture, Algorithms &amp; Projects" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Data Engineering for Large Models: Architecture, Algorithms &amp; Projects
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/datascale-ai/data_engineering_book/tree/main" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Table of Contents
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2">
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 1: Infrastructure &amp; Core Concepts
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 1: Infrastructure &amp; Core Concepts
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part1/1_1_data_change/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 1: Data Revolution in the LLM Era
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part1/1_2_data_infra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 2: Data Infrastructure Selection
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 2: Text Pre-training Data Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 2: Text Pre-training Data Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part2/2_1_data_acquisition/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 3: Data Acquisition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part2/2_2_cleaning_denoising/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 4: Cleaning &amp; Deduplication
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part2/2_3_tokenization_serialization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 5: Tokenization &amp; Serialization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 3: Multimodal Data Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 3: Multimodal Data Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_1_image_text_pairs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 6: Image-Text Pair Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Chapter 7: Recaptioning
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 7: Recaptioning
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#chapter-7-data-recaptioning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter 7: Data Recaptioning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Chapter 7: Data Recaptioning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#chapter-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter Summary
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#71-limitations-of-alt-text-why-are-raw-web-descriptions-unusable" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.1 Limitations of Alt-text: Why Are Raw Web Descriptions Unusable?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-synthetic-caption-factory-using-vlm-to-rebirth-data" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2 Synthetic Caption Factory: Using VLM to Rebirth Data
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.2 Synthetic Caption Factory: Using VLM to Rebirth Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#721-model-selection-and-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2.1 Model Selection and Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#722-prompt-strategy-controlling-granularity" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2.2 Prompt Strategy: Controlling Granularity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#723-engineering-implementation-building-high-throughput-inference-service-with-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2.3 Engineering Implementation: Building High-Throughput Inference Service with vLLM
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-ocr-enhancement-extract-and-fuse-text-from-images" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3 OCR Enhancement: Extract and Fuse Text from Images
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.3 OCR Enhancement: Extract and Fuse Text from Images">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#731-ocr-enhancement-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3.1 OCR Enhancement Pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#732-core-code-ocr-result-injection" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3.2 Core Code: OCR Result Injection
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_3_video_audio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 8: Video &amp; Audio Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5">
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 4: Alignment &amp; Synthetic Data Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 4: Alignment &amp; Synthetic Data Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part4/4_1_sft_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 9: Instruction Fine-tuning Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part4/4_2_synthetic_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 10: Synthetic Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part4/4_3_preference_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 11: Human Preference Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6">
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 5: Application-level Data Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 5: Application-level Data Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part5/5_1_rag_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 12: RAG Data Pipeline
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part5/5_2_mm_rag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 13: Multimodal RAG
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7">
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 6: Capstone Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 6: Capstone Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_1_mini_c4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 1: Building Mini-C4 Pre-training Set
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_2_legal_sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 2: Domain Expert SFT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_3_llava_instruct/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 3: Building LLaVA Multimodal Instruction Set
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_4_synthetic_textbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 4: Synthetic Math/Code Textbook
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_5_mm_rag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 5: Multimodal RAG Financial Report Assistant
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#chapter-7-data-recaptioning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter 7: Data Recaptioning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Chapter 7: Data Recaptioning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#chapter-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter Summary
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#71-limitations-of-alt-text-why-are-raw-web-descriptions-unusable" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.1 Limitations of Alt-text: Why Are Raw Web Descriptions Unusable?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-synthetic-caption-factory-using-vlm-to-rebirth-data" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2 Synthetic Caption Factory: Using VLM to Rebirth Data
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.2 Synthetic Caption Factory: Using VLM to Rebirth Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#721-model-selection-and-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2.1 Model Selection and Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#722-prompt-strategy-controlling-granularity" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2.2 Prompt Strategy: Controlling Granularity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#723-engineering-implementation-building-high-throughput-inference-service-with-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2.3 Engineering Implementation: Building High-Throughput Inference Service with vLLM
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-ocr-enhancement-extract-and-fuse-text-from-images" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3 OCR Enhancement: Extract and Fuse Text from Images
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7.3 OCR Enhancement: Extract and Fuse Text from Images">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#731-ocr-enhancement-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3.1 OCR Enhancement Pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#732-core-code-ocr-result-injection" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3.2 Core Code: OCR Result Injection
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Chapter 7: Recaptioning</h1>

<h2 id="chapter-7-data-recaptioning">Chapter 7: Data Recaptioning<a class="headerlink" href="#chapter-7-data-recaptioning" title="Permanent link">¶</a></h2>
<h3 id="chapter-summary">Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permanent link">¶</a></h3>
<p>Original Alt-text (alternative text) on the internet is essentially auxiliary content designed by web developers for Search Engine Optimization (SEO). Its core goal is to improve webpage ranking in search results, not to accurately and comprehensively describe the visual content of the image itself—this leads to a large amount of raw Alt-text that cannot meet the core requirement of "visual-text precise alignment" for Visual Language Model (VLM) training. This chapter systematically introduces how to leverage mainstream Visual Language Models (VLMs) to build an efficient, scalable "synthetic caption factory" for automatic large-scale image data recaptioning. We will explore in depth the key role of Prompt Engineering in precisely controlling description granularity (from brief to detailed), crack the differentiated description precision requirements for different downstream tasks, and introduce Optical Character Recognition (OCR) technology as a supplement to address VLM's weak recognition of text in rich-text images (such as documents, posters, charts), further enhancing model understanding of complex images.</p>
<p><strong>Learning Objectives</strong>:
* Deeply understand the essential causes of Alt-text's "three sins" (irrelevant, too short, visual omission), and the specific harms such low-quality descriptions cause to visual language model and generative visual model training (e.g., model hallucination, visual-text alignment failure, poor generalization).
* Master the complete workflow of deploying mainstream VLMs like LLaVA and CogVLM using vLLM (efficient large model inference engine), understand core principles of high-throughput inference, and achieve rapid large-scale image recaptioning.
* Be able to design layered Prompt strategies based on different downstream tasks (e.g., CLIP-style dual-tower model pre-training, Sora-style generative model training), flexibly generating brief or detailed image descriptions to precisely match task requirements.
* Master core OCR application methods, implement dynamic fusion of OCR recognition results with VLM Prompts, solve low description quality for document and poster-class rich-text images, and significantly improve description accuracy and richness for such images.</p>
<p><strong>Scenario Introduction</strong>:</p>
<blockquote>
<p>"Imagine you are training a Sora-like model. You feed the model an image of a golden retriever running in the sunset with the Eiffel Tower in the background. Yet the raw data label is 'IMG_20240501.jpg' or 'Best dog food 50% off.' With such data, the model will never learn the visual correspondence of 'golden retriever' and 'Eiffel Tower,' let alone understand 'sunset lighting.' We need 'data recaptioning'—letting AI act as annotator—to accurately write the dog and tower into the text."</p>
</blockquote>
<h3 id="71-limitations-of-alt-text-why-are-raw-web-descriptions-unusable">7.1 Limitations of Alt-text: Why Are Raw Web Descriptions Unusable?<a class="headerlink" href="#71-limitations-of-alt-text-why-are-raw-web-descriptions-unusable" title="Permanent link">¶</a></h3>
<p>In the training of visual language models and generative visual models, data quality directly determines model ceiling—and raw Alt-text from web pages is precisely one of the main sources of low-quality visual-text data. According to internal research reports from DeepMind ("Scaling Language-Image Pre-training with Weakly Supervised Image-Text Data") and OpenAI ("Training language models to follow instructions with human feedback"), directly using raw Alt-text crawled from the internet as training data causes model performance to "cap" prematurely (i.e., regardless of data volume increase, model visual understanding and text generation precision cannot improve further), or even degrade. The core problems can be summarized as "three sins":</p>
<ul>
<li><strong>Extremely Noisy</strong>: Large amounts of Alt-text contain only filenames (e.g., "IMG_20240501.jpg"), dates, irrelevant SEO keyword stacking (e.g., "buy cheap shoes nike adidas" "best coffee shop near me"). Such descriptions are completely unrelated to image visual content. Using them for training not only cannot help the model establish visual-text correspondence, but also pollutes model language ability, causing the model to generate irrelevant, redundant text, or even severe hallucinations.</li>
<li><strong>Visual Omission</strong>: Alt-text design intent is mostly to support webpage functionality, not to describe visual content—it often only describes image function (e.g., "click purchase button," "view more details") or commercial attributes (e.g., "red XL size," "limited-time discount"), while completely ignoring image visual details (e.g., object shape, color, texture, spatial relations, lighting effects). For example, an image showing "a red pure cotton T-shirt with white vintage logo on chest, laid flat on wooden table" may have Alt-text of merely "red T-shirt promotion"—such description cannot let the model learn any visual features.</li>
<li><strong>Too Short</strong>: According to Common Crawl (world's largest web crawl dataset) statistics, over 50% of Alt-text is less than 5 words, 30% even less than 3 words. Such extremely short descriptions cannot carry complex visual logic, spatial relations, and detail information—e.g., cannot describe "a golden retriever lying on grass, front paws on a red ball, background of hillside full of wildflowers" involving multiple objects, scenes, and interaction relations.</li>
</ul>
<p><strong>Value of Recaptioning</strong>: The core value of data recaptioning is to use AI to automatically generate high-quality "visual-text precise alignment" descriptions to replace low-quality raw Alt-text, breaking model performance ceiling. This has been confirmed by top industry research—OpenAI explicitly stated in the DALL-E 3 paper ("DALL·E 3: Scaling Autoregressive Image Generation with Improved Alignment") that using up to 95% synthetic long-form text (Synthetic Captions, i.e., recaption text generated by VLM) for training is one of the core reasons its instruction-following ability and visual restoration precision far surpass Stable Diffusion XL (SDXL). Synthetic long-form text can precisely capture image visual details, logical relations, and scene atmosphere, letting the model truly learn "describe what you see," thereby improving subsequent generation, recognition, and understanding capabilities.</p>
<h3 id="72-synthetic-caption-factory-using-vlm-to-rebirth-data">7.2 Synthetic Caption Factory: Using VLM to Rebirth Data<a class="headerlink" href="#72-synthetic-caption-factory-using-vlm-to-rebirth-data" title="Permanent link">¶</a></h3>
<p>To achieve large-scale image recaptioning, relying solely on manual annotation is not only extremely costly (minutes per image, datasets often millions or billions of images) but also suffers from inconsistent annotation standards and low efficiency. Therefore, we need to build a VLM-driven "synthetic caption factory"—taking raw images as input and high-quality, standardized text descriptions as output, completing data recaptioning through automation and batch processing to achieve data value "rebirth."</p>
<p>The core logic of this "factory" is: feed raw images into optimized VLM, control description granularity and style through carefully designed Prompts, then improve processing throughput through efficient inference engine, finally outputting high-quality descriptions that meet downstream task requirements. The entire flow can be divided into three core links: "Model Selection and Architecture Design," "Prompt Strategy Optimization," and "Engineering Deployment."</p>
<h4 id="721-model-selection-and-architecture">7.2.1 Model Selection and Architecture<a class="headerlink" href="#721-model-selection-and-architecture" title="Permanent link">¶</a></h4>
<p>VLM architecture directly determines description quality, speed, and applicable scenarios. Current mainstream VLM architectures are mainly divided into three types. The table below shows representative models, advantages/disadvantages comparison, and recommended scenarios for each architecture. Selection can be flexible based on downstream task requirements (e.g., description precision, processing speed, data type):</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Architecture</th>
<th style="text-align: left;">Representative Models</th>
<th style="text-align: left;">Advantages</th>
<th style="text-align: left;">Disadvantages</th>
<th style="text-align: left;">Recommended Scenarios</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Q-Former Connection</strong></td>
<td style="text-align: left;">BLIP-2, InstructBLIP</td>
<td style="text-align: left;">Small parameter count (typically billions, far below large language models), fast inference (single image inference can be tens of milliseconds), low training and deployment cost, less prone to text hallucination (description closely fits image)</td>
<td style="text-align: left;">Short description length, average detail capture, prone to "repetitive description" (repeating few core objects, lacking detail extension), limited understanding of complex scenes</td>
<td style="text-align: left;">Quick initial screening of massive images (e.g., rough recaption of billions of images to filter valuable data), or generating short Alt-text replacements (for scenarios with strict description length limits)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MLP Projection + LLM</strong></td>
<td style="text-align: left;">LLaVA-1_6 / NeXT</td>
<td style="text-align: left;">Extremely detailed descriptions, captures subtle image details (e.g., lighting, texture, object interactions), strong instruction following (precisely responds to Prompt requirements like "describe in scene order," "highlight core objects"), supports multi-turn dialogue (can optimize description quality through multi-turn Prompts)</td>
<td style="text-align: left;">Heavy logic computation (requires 7B+ parameter LLMs like LLaMA 2 7B/13B), relatively slow inference, without Prompt constraints prone to verbose, redundant descriptions</td>
<td style="text-align: left;">Main model, for generating high-quality, long-form Dense Caption (e.g., training Sora-style generative models, SD3 image generation models, scenarios requiring precise, detailed visual-text alignment data)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Vision-First Architecture</strong></td>
<td style="text-align: left;">CogVLM, Qwen-VL</td>
<td style="text-align: left;">High visual resolution (supports HD image input, some models support 4K), excels at fine-grained object recognition, especially high precision for text and small widgets (buttons, input fields) in rich-text images (documents, charts, UI screenshots), understands text-visual element associations</td>
<td style="text-align: left;">Higher VRAM usage (7B model deployment requires at least 24GB VRAM), non-standard architecture (different models have varying deployment approaches), slightly cumbersome deployment, medium inference speed</td>
<td style="text-align: left;">Specifically for documents, charts, UI screenshots, posters and other rich-text data (e.g., training models that generate document images, UI interfaces, or scenarios requiring precise text recognition in images)</td>
</tr>
</tbody>
</table>
<p>Supplementary note: Core difference among the three architectures lies in "how visual module connects to language module": Q-Former architecture uses dedicated Q-Former module to convert visual features to language-understandable vectors before input to lightweight language model; MLP projection architecture uses multi-layer perceptron (MLP) to project visual features to language model embedding space, deeply integrating with large language model; vision-first architecture strengthens visual module resolution and recognition capability, weakens language module redundant computation, prioritizing "vision understanding first."</p>
<h4 id="722-prompt-strategy-controlling-granularity">7.2.2 Prompt Strategy: Controlling Granularity<a class="headerlink" href="#722-prompt-strategy-controlling-granularity" title="Permanent link">¶</a></h4>
<p>Prompt Engineering is the "core controller" of the "synthetic caption factory"—the same VLM, under different Prompt guidance, generates completely different data distributions (description length, detail richness, style). Therefore, we need to design layered Prompt strategies based on specific downstream task requirements, precisely controlling description granularity so generated descriptions perfectly match task needs.</p>
<p>Core Principle: Prompt design must clearly specify "task instruction," "description scope," and "granularity requirements," avoiding vague expressions (e.g., only using "describe this image" leads to unstable model output). Meanwhile, adding "constraints" (e.g., "no more than 20 words," "highlight core objects and background") can further optimize output quality.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../../../images/part3/%E5%9B%BE7_1_%E7%AE%80%E7%95%A5%E4%B8%8E%E8%AF%A6%E7%BB%86%E7%9A%84Prompt%E7%AD%96%E7%95%A5.png" data-desc-position="bottom"><img alt="Figure 7-1: Brief vs Detailed Prompt Strategy" src="../../../images/part3/%E5%9B%BE7_1_%E7%AE%80%E7%95%A5%E4%B8%8E%E8%AF%A6%E7%BB%86%E7%9A%84Prompt%E7%AD%96%E7%95%A5.png"></a>
<em>Figure 7-1: Brief vs Detailed Prompt Strategy</em></p>
<p>Figure 7-1 intuitively compares output differences of two core Prompt strategies—brief Prompt generates concise descriptions with only core objects and scenes; detailed Prompt generates descriptions with rich details including object form, lighting, color, spatial relations, etc. The two strategies adapt to different downstream tasks respectively.</p>
<p>Below are two most commonly used layered Prompt strategies. Adjust flexibly based on actual needs, or design medium-granularity Prompt strategies on this basis:</p>
<p><strong>Strategy One: Brief Description (Brief Caption)</strong>
* <strong>Prompt</strong>: "Describe this image concisely in one sentence."
  Supplementary optimization Prompt (for stability): "Describe this image concisely in one sentence, focusing only on the main subject and key background, no redundant details."
* <strong>Purpose</strong>: Adapt to CLIP-style dual-tower models' (vision-text dual-tower architecture) Context Length limit—such models typically limit text input to 77 tokens or less; overly long descriptions get truncated, preventing normal learning. Also suitable for scenarios with strict description length requirements (e.g., image retrieval, quick annotation).
* <strong>Expected Output</strong>: "A golden retriever running on grass near the Eiffel Tower."
  Output characteristics: Length controlled to 10-20 words, only core objects (golden retriever), key action (running), core background (Eiffel Tower, grass), no extra details, concise and clear.</p>
<p><strong>Strategy Two: Detailed Description (Detailed Caption)</strong>
* <strong>Prompt</strong>: "Describe this image in extreme detail. Start with the main subject, then describe the background, lighting, colors, and artistic style. Mention any specific interactions between objects."
  Supplementary optimization Prompt (for better detail capture): "Describe this image in extreme detail. First, describe the main subject's appearance (shape, color, texture), then the background scene, lighting effects (brightness, color temperature), color matching, and artistic style. Finally, mention the interactions between objects and the overall atmosphere of the image."
* <strong>Purpose</strong>: Adapt to GenAI model training (e.g., Sora, SD3, Ideogram)—such models need detailed descriptions to learn image detail features, logical relations, and scene atmosphere to generate high-precision, instruction-compliant images. Also suitable for scenarios requiring precise visual-text alignment (e.g., visual QA, image editing).
* <strong>Expected Output</strong>: "A dynamic wide-angle shot of a fluffy golden retriever running joyfully across a green lawn. The dog's fur is illuminated by the warm, golden light of a setting sun, with some light brown strands glinting in the sunlight. Its ears flop backward as it runs, and its tail is raised high, showing a happy mood. In the blurred background, the iconic iron lattice structure of the Eiffel Tower rises against a gradient sky of purple and orange, with a few wispy clouds floating nearby. The lawn is dotted with small white clover flowers, and the overall atmosphere of the image is warm and lively, with soft focus on the dog and a blurred background that highlights the main subject."
  Output characteristics: Length typically 50-200 words, covering core object details, background scene, lighting, color, artistic style, object interactions, and overall atmosphere—detail-rich, high visual-text alignment precision.</p>
<p>Supplementary tip: Besides the above two strategies, "task-oriented Prompts" can be designed, e.g., for e-commerce images ("Describe this product image in detail, focusing on the product's appearance, color, size, texture, and placement, suitable for e-commerce promotion"), for document images ("Describe this document image in detail, including the text content, layout, font style, and color of the text"), to further improve description relevance.</p>
<h4 id="723-engineering-implementation-building-high-throughput-inference-service-with-vllm">7.2.3 Engineering Implementation: Building High-Throughput Inference Service with vLLM<a class="headerlink" href="#723-engineering-implementation-building-high-throughput-inference-service-with-vllm" title="Permanent link">¶</a></h4>
<p>For large-scale data recaptioning (e.g., processing billion-scale image datasets), ordinary HuggingFace <code>generate()</code> is far insufficient—slow inference, low throughput, unable to efficiently utilize GPU resources. A single GPU can only process thousands of images per day; large-scale processing would consume significant time and hardware cost. Therefore, we need dedicated large model inference engine—vLLM, which supports PagedAttention and Continuous Batching, two core optimization techniques that can improve VLM inference throughput 3-5x while reducing GPU VRAM usage, achieving efficient, large-scale image recaptioning.</p>
<p>vLLM is a high-efficiency large model inference engine developed by UC Berkeley research team. Core advantages are "high throughput, low latency, high GPU utilization," perfectly suited for deploying LLaVA, CogVLM and other mainstream VLMs, with API interface compatible with HuggingFace and minimal migration cost.</p>
<p>Below is the core code for deploying LLaVA-1_5-7b-hf with vLLM for high-throughput image recaptioning, including model initialization, Prompt template design, batch processing, and output extraction—complete workflow with key parameter interpretation and optimization tips:</p>
<p></p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>  <span class="c1"># For displaying batch processing progress</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="c1"># Initialize vLLM inference engine</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="c1"># tensor_parallel_size=4: Use 4 GPUs for tensor parallelism, for large models (7B/13B) deployment, adjust based on GPU count (1, 2, 4, 8)</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="c1"># Note: Tensor parallelism requires multiple same-model GPUs with NVLink support for faster data transfer</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="c1"># trust_remote_code=True: Allow loading LLaVA custom code (e.g., vision-language fusion module), as LLaVA architecture is non-standard HuggingFace</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="c1"># model: Model name from HuggingFace Hub (e.g., llava-hf/llava-1_5-7b-hf, llava-hf/llava-1_5-13b-hf)</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="c1"># gpu_memory_utilization=0_9: Set GPU VRAM utilization to 90%, balance throughput and stability, avoid OOM</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    <span class="n">model</span><span class="o">=</span><span class="s2">"llava-hf/llava-1_5-7b-hf"</span><span class="p">,</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="n">gpu_memory_utilization</span><span class="o">=</span><span class="mi">0_9</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="p">)</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="c1"># Define Prompt template (LLaVA requires specific dialogue format, otherwise affects instruction following)</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="c1"># Tip: Adding "Analyze the image" in Prompt often works better than "Describe the image"</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="c1"># because "Analyze" guides model to observe image details more carefully, reducing perfunctory descriptions</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a><span class="c1"># Here using detailed description template, can replace with brief description template as needed</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="n">prompt_template</span> <span class="o">=</span> <span class="s2">"USER: &lt;image&gt;</span><span class="se">\n</span><span class="s2">Analyze this image and describe it in extreme detail. Start with the main subject, then describe the background, lighting, colors, and artistic style. Mention any specific interactions between objects. ASSISTANT:"</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a><span class="c1"># Configure sampling parameters for description quality and stability</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a><span class="c1"># temperature=0_2: Lower randomness (0-1 range), lower temperature = more stable, image-fitting descriptions, fewer hallucinations</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a><span class="c1"># For more diverse descriptions, adjust to 0_5-0_7; if too high (&gt;0_8), likely to produce image-unrelated hallucinations</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a><span class="c1"># max_tokens=256: Limit output length, prevent overly verbose descriptions</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a><span class="c1"># top_p=0_95: Nucleus sampling, keep only tokens with cumulative probability to 95%, further reduce hallucination risk</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>    <span class="n">temperature</span><span class="o">=</span><span class="mi">0_2</span><span class="p">,</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>    <span class="n">top_p</span><span class="o">=</span><span class="mi">0_95</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a><span class="p">)</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a><span class="k">def</span><span class="w"> </span><span class="nf">load_image_batch</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a><span class="w">    </span><span class="sd">"""</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a><span class="sd">    Batch load images for efficient processing</span>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a><span class="sd">    image_dir: Image folder path, all images in this folder</span>
<a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a><span class="sd">    batch_size: Images per batch, adjust based on GPU VRAM (16, 32, 64), larger VRAM = larger batch_size</span>
<a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a><span class="sd">    return: Batch image list (PIL.Image format) and corresponding image path list</span>
<a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a><span class="sd">    """</span>
<a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a>    <span class="n">image_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">image_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">((</span><span class="s1">'jpg'</span><span class="p">,</span> <span class="s1">'png'</span><span class="p">,</span> <span class="s1">'jpeg'</span><span class="p">))]</span>
<a id="__codelineno-0-44" name="__codelineno-0-44" href="#__codelineno-0-44"></a>    <span class="n">image_batches</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-45" name="__codelineno-0-45" href="#__codelineno-0-45"></a>    <span class="n">path_batches</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-46" name="__codelineno-0-46" href="#__codelineno-0-46"></a>
<a id="__codelineno-0-47" name="__codelineno-0-47" href="#__codelineno-0-47"></a>    <span class="c1"># Load images in batches</span>
<a id="__codelineno-0-48" name="__codelineno-0-48" href="#__codelineno-0-48"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_paths</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
<a id="__codelineno-0-49" name="__codelineno-0-49" href="#__codelineno-0-49"></a>        <span class="n">batch_paths</span> <span class="o">=</span> <span class="n">image_paths</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
<a id="__codelineno-0-50" name="__codelineno-0-50" href="#__codelineno-0-50"></a>        <span class="n">batch_images</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-51" name="__codelineno-0-51" href="#__codelineno-0-51"></a>        <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">batch_paths</span><span class="p">:</span>
<a id="__codelineno-0-52" name="__codelineno-0-52" href="#__codelineno-0-52"></a>            <span class="k">try</span><span class="p">:</span>
<a id="__codelineno-0-53" name="__codelineno-0-53" href="#__codelineno-0-53"></a>                <span class="c1"># Load image and convert to RGB (avoid model errors from grayscale/transparent images)</span>
<a id="__codelineno-0-54" name="__codelineno-0-54" href="#__codelineno-0-54"></a>                <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">'RGB'</span><span class="p">)</span>
<a id="__codelineno-0-55" name="__codelineno-0-55" href="#__codelineno-0-55"></a>                <span class="n">batch_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<a id="__codelineno-0-56" name="__codelineno-0-56" href="#__codelineno-0-56"></a>            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<a id="__codelineno-0-57" name="__codelineno-0-57" href="#__codelineno-0-57"></a>                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Failed to load image </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-0-58" name="__codelineno-0-58" href="#__codelineno-0-58"></a>                <span class="k">continue</span>
<a id="__codelineno-0-59" name="__codelineno-0-59" href="#__codelineno-0-59"></a>        <span class="k">if</span> <span class="n">batch_images</span><span class="p">:</span>  <span class="c1"># Skip empty batches</span>
<a id="__codelineno-0-60" name="__codelineno-0-60" href="#__codelineno-0-60"></a>            <span class="n">image_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_images</span><span class="p">)</span>
<a id="__codelineno-0-61" name="__codelineno-0-61" href="#__codelineno-0-61"></a>            <span class="n">path_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_paths</span><span class="p">)</span>
<a id="__codelineno-0-62" name="__codelineno-0-62" href="#__codelineno-0-62"></a>    <span class="k">return</span> <span class="n">image_batches</span><span class="p">,</span> <span class="n">path_batches</span>
<a id="__codelineno-0-63" name="__codelineno-0-63" href="#__codelineno-0-63"></a>
<a id="__codelineno-0-64" name="__codelineno-0-64" href="#__codelineno-0-64"></a><span class="k">def</span><span class="w"> </span><span class="nf">process_batch</span><span class="p">(</span><span class="n">image_batch</span><span class="p">):</span>
<a id="__codelineno-0-65" name="__codelineno-0-65" href="#__codelineno-0-65"></a><span class="w">    </span><span class="sd">"""</span>
<a id="__codelineno-0-66" name="__codelineno-0-66" href="#__codelineno-0-66"></a><span class="sd">    Process a batch of images, generate corresponding recaption text</span>
<a id="__codelineno-0-67" name="__codelineno-0-67" href="#__codelineno-0-67"></a><span class="sd">    image_batch: List[PIL.Image], batch image list</span>
<a id="__codelineno-0-68" name="__codelineno-0-68" href="#__codelineno-0-68"></a><span class="sd">    return: List[str], recaption text list for each image</span>
<a id="__codelineno-0-69" name="__codelineno-0-69" href="#__codelineno-0-69"></a><span class="sd">    """</span>
<a id="__codelineno-0-70" name="__codelineno-0-70" href="#__codelineno-0-70"></a>    <span class="c1"># Generate corresponding Prompt for each image</span>
<a id="__codelineno-0-71" name="__codelineno-0-71" href="#__codelineno-0-71"></a>    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">prompt_template</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">image_batch</span><span class="p">))]</span>
<a id="__codelineno-0-72" name="__codelineno-0-72" href="#__codelineno-0-72"></a>
<a id="__codelineno-0-73" name="__codelineno-0-73" href="#__codelineno-0-73"></a>    <span class="c1"># vLLM supports direct multi_modal_data input, no manual image format conversion needed</span>
<a id="__codelineno-0-74" name="__codelineno-0-74" href="#__codelineno-0-74"></a>    <span class="c1"># This step is non-blocking; vLLM internally does Continuous Batching for efficient GPU utilization</span>
<a id="__codelineno-0-75" name="__codelineno-0-75" href="#__codelineno-0-75"></a>    <span class="c1"># When one batch completes partially, immediately load next batch partial data to avoid GPU idle</span>
<a id="__codelineno-0-76" name="__codelineno-0-76" href="#__codelineno-0-76"></a>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<a id="__codelineno-0-77" name="__codelineno-0-77" href="#__codelineno-0-77"></a>        <span class="n">prompts</span><span class="p">,</span> 
<a id="__codelineno-0-78" name="__codelineno-0-78" href="#__codelineno-0-78"></a>        <span class="n">sampling_params</span><span class="p">,</span> 
<a id="__codelineno-0-79" name="__codelineno-0-79" href="#__codelineno-0-79"></a>        <span class="n">multi_modal_data</span><span class="o">=</span><span class="p">{</span><span class="s2">"image"</span><span class="p">:</span> <span class="n">image_batch</span><span class="p">}</span>
<a id="__codelineno-0-80" name="__codelineno-0-80" href="#__codelineno-0-80"></a>    <span class="p">)</span>
<a id="__codelineno-0-81" name="__codelineno-0-81" href="#__codelineno-0-81"></a>
<a id="__codelineno-0-82" name="__codelineno-0-82" href="#__codelineno-0-82"></a>    <span class="c1"># Extract generated description text, remove Prompt part, keep only model response</span>
<a id="__codelineno-0-83" name="__codelineno-0-83" href="#__codelineno-0-83"></a>    <span class="n">captions</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-84" name="__codelineno-0-84" href="#__codelineno-0-84"></a>    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
<a id="__codelineno-0-85" name="__codelineno-0-85" href="#__codelineno-0-85"></a>        <span class="c1"># Extract content after ASSISTANT: as model-generated description</span>
<a id="__codelineno-0-86" name="__codelineno-0-86" href="#__codelineno-0-86"></a>        <span class="n">caption</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">"ASSISTANT:"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<a id="__codelineno-0-87" name="__codelineno-0-87" href="#__codelineno-0-87"></a>        <span class="n">captions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">caption</span><span class="p">)</span>
<a id="__codelineno-0-88" name="__codelineno-0-88" href="#__codelineno-0-88"></a>    <span class="k">return</span> <span class="n">captions</span>
<a id="__codelineno-0-89" name="__codelineno-0-89" href="#__codelineno-0-89"></a>
<a id="__codelineno-0-90" name="__codelineno-0-90" href="#__codelineno-0-90"></a><span class="k">def</span><span class="w"> </span><span class="nf">save_captions</span><span class="p">(</span><span class="n">image_paths</span><span class="p">,</span> <span class="n">captions</span><span class="p">,</span> <span class="n">save_path</span><span class="p">):</span>
<a id="__codelineno-0-91" name="__codelineno-0-91" href="#__codelineno-0-91"></a><span class="w">    </span><span class="sd">"""</span>
<a id="__codelineno-0-92" name="__codelineno-0-92" href="#__codelineno-0-92"></a><span class="sd">    Save recaption text corresponding to image paths for subsequent use (e.g., model training)</span>
<a id="__codelineno-0-93" name="__codelineno-0-93" href="#__codelineno-0-93"></a><span class="sd">    image_paths: Image path list</span>
<a id="__codelineno-0-94" name="__codelineno-0-94" href="#__codelineno-0-94"></a><span class="sd">    captions: Recaption text list</span>
<a id="__codelineno-0-95" name="__codelineno-0-95" href="#__codelineno-0-95"></a><span class="sd">    save_path: Save file path (txt format)</span>
<a id="__codelineno-0-96" name="__codelineno-0-96" href="#__codelineno-0-96"></a><span class="sd">    """</span>
<a id="__codelineno-0-97" name="__codelineno-0-97" href="#__codelineno-0-97"></a>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<a id="__codelineno-0-98" name="__codelineno-0-98" href="#__codelineno-0-98"></a>        <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">cap</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">image_paths</span><span class="p">,</span> <span class="n">captions</span><span class="p">):</span>
<a id="__codelineno-0-99" name="__codelineno-0-99" href="#__codelineno-0-99"></a>            <span class="c1"># Format: image_path\trecaption_text, for easy reading and parsing</span>
<a id="__codelineno-0-100" name="__codelineno-0-100" href="#__codelineno-0-100"></a>            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">cap</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-0-101" name="__codelineno-0-101" href="#__codelineno-0-101"></a>
<a id="__codelineno-0-102" name="__codelineno-0-102" href="#__codelineno-0-102"></a><span class="c1"># Main function: batch process images and generate recaptions</span>
<a id="__codelineno-0-103" name="__codelineno-0-103" href="#__codelineno-0-103"></a><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
<a id="__codelineno-0-104" name="__codelineno-0-104" href="#__codelineno-0-104"></a>    <span class="n">image_dir</span> <span class="o">=</span> <span class="s2">"path/to/your/image/directory"</span>  <span class="c1"># Replace with your image folder path</span>
<a id="__codelineno-0-105" name="__codelineno-0-105" href="#__codelineno-0-105"></a>    <span class="n">save_path</span> <span class="o">=</span> <span class="s2">"recaption_results.txt"</span>        <span class="c1"># Recaption results save path</span>
<a id="__codelineno-0-106" name="__codelineno-0-106" href="#__codelineno-0-106"></a>    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>                            <span class="c1"># Images per batch, adjust based on GPU VRAM</span>
<a id="__codelineno-0-107" name="__codelineno-0-107" href="#__codelineno-0-107"></a>
<a id="__codelineno-0-108" name="__codelineno-0-108" href="#__codelineno-0-108"></a>    <span class="c1"># Load image batches</span>
<a id="__codelineno-0-109" name="__codelineno-0-109" href="#__codelineno-0-109"></a>    <span class="n">image_batches</span><span class="p">,</span> <span class="n">path_batches</span> <span class="o">=</span> <span class="n">load_image_batch</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<a id="__codelineno-0-110" name="__codelineno-0-110" href="#__codelineno-0-110"></a>
<a id="__codelineno-0-111" name="__codelineno-0-111" href="#__codelineno-0-111"></a>    <span class="c1"># Batch process and save results</span>
<a id="__codelineno-0-112" name="__codelineno-0-112" href="#__codelineno-0-112"></a>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<a id="__codelineno-0-113" name="__codelineno-0-113" href="#__codelineno-0-113"></a>        <span class="k">for</span> <span class="n">img_batch</span><span class="p">,</span> <span class="n">path_batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">image_batches</span><span class="p">,</span> <span class="n">path_batches</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">image_batches</span><span class="p">)):</span>
<a id="__codelineno-0-114" name="__codelineno-0-114" href="#__codelineno-0-114"></a>            <span class="n">captions</span> <span class="o">=</span> <span class="n">process_batch</span><span class="p">(</span><span class="n">img_batch</span><span class="p">)</span>
<a id="__codelineno-0-115" name="__codelineno-0-115" href="#__codelineno-0-115"></a>            <span class="c1"># Write current batch results</span>
<a id="__codelineno-0-116" name="__codelineno-0-116" href="#__codelineno-0-116"></a>            <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">cap</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">path_batch</span><span class="p">,</span> <span class="n">captions</span><span class="p">):</span>
<a id="__codelineno-0-117" name="__codelineno-0-117" href="#__codelineno-0-117"></a>                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">cap</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-0-118" name="__codelineno-0-118" href="#__codelineno-0-118"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Recaptioning complete, results saved to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
Supplementary engineering optimization tips:<p></p>
<ul>
<li><strong>Image Preprocessing</strong>: When batch loading, uniformly resize images (e.g., to 224×224 or 448×448) to avoid model inference speed fluctuation and VRAM instability from image size differences; normalize images to improve description precision.</li>
<li><strong>Error Handling</strong>: Add exception handling for image load failure and model inference failure to avoid batch processing interruption; for failed images, record path and process separately.</li>
<li><strong>Hardware Optimization</strong>: Prefer NVIDIA A100, A800 for deployment; VRAM at least 24GB (7B model); for very large scale, use GPU cluster and vLLM distributed inference for higher throughput.</li>
<li><strong>Prompt Caching</strong>: For same-type images (e.g., batch e-commerce posters), cache Prompt template to avoid repeated generation and improve processing speed.</li>
</ul>
<h3 id="73-ocr-enhancement-extract-and-fuse-text-from-images">7.3 OCR Enhancement: Extract and Fuse Text from Images<a class="headerlink" href="#73-ocr-enhancement-extract-and-fuse-text-from-images" title="Permanent link">¶</a></h3>
<p>While ordinary VLMs have certain visual understanding and can recognize objects, scenes, and simple text in images, they face two core problems with dense-text images (documents, posters, charts, PDF screenshots): first, low text recognition precision, prone to misrecognition and omission (especially artistic fonts, blurry text); second, unable to effectively associate text with visual elements, causing descriptions to omit text meaning and role.</p>
<p>For example, an e-commerce poster with large text "Summer Sale 50% Off" may get only "A red promotional poster" from ordinary VLM—completely ignoring text. Even when text is recognized, errors like "Summer Sale 30% Off" may occur. Yet text is crucial for recaptioning such images—it directly determines core meaning and purpose.</p>
<p>Best practice is to introduce dedicated OCR engine (e.g., PaddleOCR, Tesseract) as VLM's "external brain." Use OCR to precisely extract image text, then dynamically fuse with VLM Prompt, letting VLM combine text to generate more accurate, richer descriptions—significantly improving recaption quality for document and poster-class rich-text images.</p>
<p>OCR (Optical Character Recognition) technology's core is converting printed and handwritten text in images to editable text. Its text recognition precision far exceeds ordinary VLM, especially for dense text and complex font scenarios. Currently, the most widely used industrial, open-source, free, and high-precision OCR engine is PaddleOCR (Baidu PaddlePaddle open-source OCR). It supports multilingual, multi-font, blurry text recognition, fast inference, simple deployment, GPU acceleration—very suitable for combination with VLM.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../../../images/part3/%E5%9B%BE7_2_OCR%E5%A2%9E%E5%BC%BA%E6%B5%81%E6%B0%B4%E7%BA%BF.png" data-desc-position="bottom"><img alt="Figure 7-2: OCR Enhancement Pipeline" src="../../../images/part3/%E5%9B%BE7_2_OCR%E5%A2%9E%E5%BC%BA%E6%B5%81%E6%B0%B4%E7%BA%BF.png"></a>
<em>Figure 7-2: OCR Enhancement Pipeline</em></p>
<p><strong>Chart Core Interpretation</strong>: Figure 7-2 shows the complete OCR-enhanced VLM recaption flow. Core is "OCR extract text → context construction → Prompt fusion → VLM generate description," supplementing VLM's text recognition weakness with OCR to achieve "visual details + text information" dual-precise description.</p>
<h4 id="731-ocr-enhancement-pipeline">7.3.1 OCR Enhancement Pipeline<a class="headerlink" href="#731-ocr-enhancement-pipeline" title="Permanent link">¶</a></h4>
<p>OCR enhancement core is organically fusing OCR-extracted text with VLM Prompt, not simple concatenation. The entire pipeline has three core steps, each with clear optimization direction to ensure text effectively improves recaption quality:</p>
<ol>
<li><strong>Detection and Recognition</strong>: Use PaddleOCR to process raw image—first detect all text regions (Bounding Box for text position), then recognize each region, output recognized text and confidence (0-1, higher = more accurate). Core goal: "precise text extraction, filter wrong recognition"—filter low-confidence results to avoid misleading VLM.</li>
<li><strong>Context Construction</strong>: Concatenate all valid OCR text (after filtering low confidence) by actual image position (top-to-bottom, left-to-right, multi-column by column order) to build human-readable text context. Optionally classify text (e.g., title, body, button text) for VLM to understand hierarchy and role. E.g., poster text "Summer Sale" (title), "50% Off" (subtitle), "June 1 - June 10" (time) concatenates to "Summer Sale, 50% Off, June 1 - June 10" with title/body labels.</li>
<li><strong>Prompt Fusion</strong>: Naturally integrate constructed text context into VLM Prompt, explicitly telling VLM "the image contains these texts, please describe combining text and visual elements," guiding VLM to associate text with visuals (position, color, font style, text meaning vs. scene). Key is "natural fusion, no redundancy"—avoid awkwardly appending text to Prompt end, causing VLM to ignore visual details.</li>
</ol>
<p><strong>Supplementary Note</strong>: Pipeline optimization focuses on "confidence filtering" and "Prompt fusion"—without filtering low-confidence text, wrong text misleads VLM; if Prompt fusion is awkward, VLM separates text from visuals, failing to achieve true enhancement.</p>
<h4 id="732-core-code-ocr-result-injection">7.3.2 Core Code: OCR Result Injection<a class="headerlink" href="#732-core-code-ocr-result-injection" title="Permanent link">¶</a></h4>
<p>Below is the core code for using PaddleOCR to extract image text and dynamically fuse into VLM Prompt. It can seamlessly integrate with Section 7.2.3 vLLM batch processing for OCR-enhanced large-scale image recaptioning. The code includes complete logic for text extraction, confidence filtering, context construction, Prompt fusion, plus key parameter interpretation and optimization tips:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">paddleocr</span><span class="w"> </span><span class="kn">import</span> <span class="n">PaddleOCR</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="c1"># Initialize OCR engine (recommend GPU for speed; set use_gpu=False if no GPU)</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="c1"># use_angle_cls=True: Enable text direction detection, supports tilted text (e.g., tilted poster, rotated documents) to avoid recognition errors</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="c1"># lang='en': English recognition; for Chinese set lang='ch'; supports Chinese-English mixed (lang='ch_en')</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="c1"># det_model_dir, rec_model_dir: Can specify OCR detection/recognition model paths; auto-download if not specified</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span class="c1"># gpu_mem=500: GPU VRAM limit (MB), adjust based on GPU</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span class="n">ocr</span> <span class="o">=</span> <span class="n">PaddleOCR</span><span class="p">(</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>    <span class="n">use_angle_cls</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    <span class="n">lang</span><span class="o">=</span><span class="s1">'en'</span><span class="p">,</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>    <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>    <span class="n">gpu_mem</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a><span class="k">def</span><span class="w"> </span><span class="nf">generate_ocr_enhanced_prompt</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">base_prompt</span><span class="o">=</span><span class="s2">"Describe this image in detail."</span><span class="p">):</span>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a><span class="w">    </span><span class="sd">"""</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a><span class="sd">    Generate OCR-enhanced VLM Prompt, integrating OCR-extracted text into Prompt</span>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a><span class="sd">    image_path: Raw image path</span>
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span class="sd">    base_prompt: Base Prompt (e.g., brief/detailed description) as Prompt body</span>
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a><span class="sd">    return: Complete OCR-enhanced Prompt; if no valid text, return base Prompt</span>
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a><span class="sd">    """</span>
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a>    <span class="c1"># Step 1: Run OCR, extract image text and confidence</span>
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>    <span class="c1"># result is nested list: [[[[x1,y1], [x2,y2], [x3,y3], [x4,y4]], [text, confidence]], ...]</span>
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>    <span class="c1"># [x1,y1]~[x4,y4] are Bounding Box coordinates (top-left, top-right, bottom-right, bottom-left)</span>
<a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a>    <span class="c1"># text is recognized content, confidence is recognition confidence</span>
<a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a>    <span class="n">result</span> <span class="o">=</span> <span class="n">ocr</span><span class="o">.</span><span class="n">ocr</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="bp">cls</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>
<a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a>    <span class="c1"># Handle OCR result: if no text or empty, fall back to base Prompt</span>
<a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="n">result</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a>        <span class="k">return</span> <span class="sa">f</span><span class="s2">"USER: &lt;image&gt;</span><span class="se">\n</span><span class="si">{</span><span class="n">base_prompt</span><span class="si">}</span><span class="se">\n</span><span class="s2">ASSISTANT:"</span>
<a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a>
<a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a>    <span class="c1"># Step 2: Extract valid text (filter low confidence), build text context</span>
<a id="__codelineno-1-34" name="__codelineno-1-34" href="#__codelineno-1-34"></a>    <span class="n">detected_texts</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-1-35" name="__codelineno-1-35" href="#__codelineno-1-35"></a>    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<a id="__codelineno-1-36" name="__codelineno-1-36" href="#__codelineno-1-36"></a>        <span class="n">text</span> <span class="o">=</span> <span class="n">line</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Recognized text</span>
<a id="__codelineno-1-37" name="__codelineno-1-37" href="#__codelineno-1-37"></a>        <span class="n">confidence</span> <span class="o">=</span> <span class="n">line</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Recognition confidence</span>
<a id="__codelineno-1-38" name="__codelineno-1-38" href="#__codelineno-1-38"></a>        <span class="c1"># Filter results below 0_8 confidence (threshold adjustable, 0_7-0_9 based on image text clarity)</span>
<a id="__codelineno-1-39" name="__codelineno-1-39" href="#__codelineno-1-39"></a>        <span class="c1"># Also filter empty text and meaningless garbage (e.g., symbols only, spaces)</span>
<a id="__codelineno-1-40" name="__codelineno-1-40" href="#__codelineno-1-40"></a>        <span class="k">if</span> <span class="n">confidence</span> <span class="o">&gt;</span> <span class="mi">0_8</span> <span class="ow">and</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-1-41" name="__codelineno-1-41" href="#__codelineno-1-41"></a>            <span class="n">detected_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
<a id="__codelineno-1-42" name="__codelineno-1-42" href="#__codelineno-1-42"></a>
<a id="__codelineno-1-43" name="__codelineno-1-43" href="#__codelineno-1-43"></a>    <span class="c1"># Build text context: concatenate by recognition order, comma-separated, human-readable</span>
<a id="__codelineno-1-44" name="__codelineno-1-44" href="#__codelineno-1-44"></a>    <span class="n">ocr_context</span> <span class="o">=</span> <span class="s2">", "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">detected_texts</span><span class="p">)</span>
<a id="__codelineno-1-45" name="__codelineno-1-45" href="#__codelineno-1-45"></a>
<a id="__codelineno-1-46" name="__codelineno-1-46" href="#__codelineno-1-46"></a>    <span class="c1"># Step 3: Dynamically fuse OCR result with base Prompt, generate enhanced Prompt</span>
<a id="__codelineno-1-47" name="__codelineno-1-47" href="#__codelineno-1-47"></a>    <span class="c1"># Key technique: Tell model "I have detected these texts..." so model knows this is image text</span>
<a id="__codelineno-1-48" name="__codelineno-1-48" href="#__codelineno-1-48"></a>    <span class="c1"># and guide model to associate text with visuals (position, color, font, text meaning vs. scene)</span>
<a id="__codelineno-1-49" name="__codelineno-1-49" href="#__codelineno-1-49"></a>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ocr_context</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># Only enhance when text long enough (&gt;10 chars) to avoid redundancy</span>
<a id="__codelineno-1-50" name="__codelineno-1-50" href="#__codelineno-1-50"></a>        <span class="n">enhanced_prompt</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-1-51" name="__codelineno-1-51" href="#__codelineno-1-51"></a>            <span class="sa">f</span><span class="s2">"USER: &lt;image&gt;</span><span class="se">\n</span><span class="s2">"</span>
<a id="__codelineno-1-52" name="__codelineno-1-52" href="#__codelineno-1-52"></a>            <span class="sa">f</span><span class="s2">"I have detected these text segments in the image: '</span><span class="si">{</span><span class="n">ocr_context</span><span class="si">}</span><span class="s2">'. "</span>
<a id="__codelineno-1-53" name="__codelineno-1-53" href="#__codelineno-1-53"></a>            <span class="sa">f</span><span class="s2">"Using this text as a reference, describe the image in detail, "</span>
<a id="__codelineno-1-54" name="__codelineno-1-54" href="#__codelineno-1-54"></a>            <span class="sa">f</span><span class="s2">"paying attention to how the text relates to the visual elements (such as the position, color, and font style of the text, "</span>
<a id="__codelineno-1-55" name="__codelineno-1-55" href="#__codelineno-1-55"></a>            <span class="sa">f</span><span class="s2">"and the connection between the text content and the image scene). </span><span class="si">{</span><span class="n">base_prompt</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span>
<a id="__codelineno-1-56" name="__codelineno-1-56" href="#__codelineno-1-56"></a>            <span class="sa">f</span><span class="s2">"ASSISTANT:"</span>
<a id="__codelineno-1-57" name="__codelineno-1-57" href="#__codelineno-1-57"></a>        <span class="p">)</span>
<a id="__codelineno-1-58" name="__codelineno-1-58" href="#__codelineno-1-58"></a>        <span class="k">return</span> <span class="n">enhanced_prompt</span>
<a id="__codelineno-1-59" name="__codelineno-1-59" href="#__codelineno-1-59"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-1-60" name="__codelineno-1-60" href="#__codelineno-1-60"></a>        <span class="c1"># If text too short (1-2 words), don't enhance, avoid redundancy, fall back to base Prompt</span>
<a id="__codelineno-1-61" name="__codelineno-1-61" href="#__codelineno-1-61"></a>        <span class="k">return</span> <span class="sa">f</span><span class="s2">"USER: &lt;image&gt;</span><span class="se">\n</span><span class="si">{</span><span class="n">base_prompt</span><span class="si">}</span><span class="se">\n</span><span class="s2">ASSISTANT:"</span>
<a id="__codelineno-1-62" name="__codelineno-1-62" href="#__codelineno-1-62"></a>
<a id="__codelineno-1-63" name="__codelineno-1-63" href="#__codelineno-1-63"></a><span class="c1"># Test code: verify OCR-enhanced Prompt generation</span>
<a id="__codelineno-1-64" name="__codelineno-1-64" href="#__codelineno-1-64"></a><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
<a id="__codelineno-1-65" name="__codelineno-1-65" href="#__codelineno-1-65"></a>    <span class="c1"># Test image path (replace with your rich-text image, e.g., poster, document screenshot)</span>
<a id="__codelineno-1-66" name="__codelineno-1-66" href="#__codelineno-1-66"></a>    <span class="n">test_image_path</span> <span class="o">=</span> <span class="s2">"path/to/your/test/poster.jpg"</span>
<a id="__codelineno-1-67" name="__codelineno-1-67" href="#__codelineno-1-67"></a>    <span class="c1"># Base Prompt (detailed description template)</span>
<a id="__codelineno-1-68" name="__codelineno-1-68" href="#__codelineno-1-68"></a>    <span class="n">base_prompt</span> <span class="o">=</span> <span class="s2">"Describe this image in extreme detail. Start with the main subject, then describe the background, lighting, colors, and artistic style."</span>
<a id="__codelineno-1-69" name="__codelineno-1-69" href="#__codelineno-1-69"></a>    <span class="c1"># Generate enhanced Prompt</span>
<a id="__codelineno-1-70" name="__codelineno-1-70" href="#__codelineno-1-70"></a>    <span class="n">enhanced_prompt</span> <span class="o">=</span> <span class="n">generate_ocr_enhanced_prompt</span><span class="p">(</span><span class="n">test_image_path</span><span class="p">,</span> <span class="n">base_prompt</span><span class="p">)</span>
<a id="__codelineno-1-71" name="__codelineno-1-71" href="#__codelineno-1-71"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">"OCR-enhanced Prompt:"</span><span class="p">)</span>
<a id="__codelineno-1-72" name="__codelineno-1-72" href="#__codelineno-1-72"></a>    <span class="nb">print</span><span class="p">(</span><span class="n">enhanced_prompt</span><span class="p">)</span>
</code></pre></div>
<p><strong>Supplementary Optimization Tips:</strong></p>
<ul>
<li><strong>Confidence Threshold Adjustment</strong>: For clear-text images (HD documents, formal posters), adjust to 0_8-0_9 to filter few errors; for blurry, complex-font images (old posters, handwriting), adjust to 0_7-0_8 to avoid missing valid text.</li>
<li><strong>Text Context Optimization</strong>: For multi-column, hierarchical text (title, body, footnote), use Bounding Box coordinates to classify and concatenate, e.g., "Title: Summer Sale; Body: 50% Off, June 1 - June 10; Footnote: Final interpretation right reserved," for clearer VLM text hierarchy understanding.</li>
<li><strong>Prompt Fusion Optimization</strong>: Adjust fusion wording by image type—document images add "Describe the layout of the text and the relationship between the text and the document structure," poster images add "Describe the font style of the text and the role of the text in the promotional scene" for more targeted descriptions.</li>
<li><strong>Multi-OCR Engine Fusion</strong>: For extremely high precision requirements, use both PaddleOCR and Tesseract, take intersection of results for higher text recognition precision.</li>
</ul>
<p><strong>Practical Benefits</strong>: OCR enhancement significantly improves recaption quality for rich-text images. Typical comparison:</p>
<ul>
<li><strong>Ordinary VLM (no OCR) on e-commerce poster</strong>: "A red promotional poster with a white background, featuring some vague text and a button at the bottom."</li>
<li><strong>OCR-enhanced VLM</strong>: "A promotional red poster with a white background, featuring the text 'SUMMER SALE 50% OFF' in large white bold letters at the top center of the poster, and 'Shop Now' in a small blue button at the bottom right. The text 'SUMMER SALE' is in a decorative font, with a yellow shadow effect that makes it stand out. The overall layout is simple and eye-catching, focusing on highlighting the promotional information. The background is plain white, which makes the red poster and white text more prominent."</li>
</ul>
<p>This difference is crucial for training models that generate accurate text (Ideogram, SD3, document generation models)—recaptions with precise text let models learn "visual presentation of text" and "text-scene association," generating more compliant images. For visual QA and image retrieval, OCR-enhanced descriptions also improve task precision and model understanding of image core meaning.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../3_1_image_text_pairs/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Chapter 6: Image-Text Pair Processing">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Chapter 6: Image-Text Pair Processing
              </div>
            </div>
          </a>
        
        
          
          <a href="../3_3_video_audio/" class="md-footer__link md-footer__link--next" aria-label="Next: Chapter 8: Video &amp; Audio Data">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Chapter 8: Video &amp; Audio Data
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": ["navigation.sections", "navigation.expand", "navigation.indexes", "navigation.top", "navigation.footer", "toc.follow", "search.suggest", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>