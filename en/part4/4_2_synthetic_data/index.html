<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="大模型数据工程：架构、算法及项目实战">
      
      
        <meta name="author" content="ustc">
      
      
        <link rel="canonical" href="https://datascale-ai.github.io/data_engineering_book/en/part4/4_2_synthetic_data/">
      
      
        <link rel="prev" href="../4_1_sft_data/">
      
      
        <link rel="next" href="../4_3_preference_data/">
      
      
        
          <link rel="alternate" href="../../../part4/4_2_synthetic_data/" hreflang="zh">
        
          <link rel="alternate" href="./" hreflang="en">
        
          <link rel="alternate" href="../../../ja/part4/4_2_synthetic_data/" hreflang="ja">
        
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>Chapter 10: Synthetic Data - Data Engineering for Large Models: Architecture, Algorithms &amp; Projects</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-10-synthetic-data-from-data-mining-to-data-farming" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../" title="Data Engineering for Large Models: Architecture, Algorithms &amp; Projects" class="md-header__button md-logo" aria-label="Data Engineering for Large Models: Architecture, Algorithms &amp; Projects" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Data Engineering for Large Models: Architecture, Algorithms &amp; Projects
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chapter 10: Synthetic Data
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="red" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="red" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"></path></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../../part4/4_2_synthetic_data/" hreflang="zh" class="md-select__link">
              简体中文
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../../ja/part4/4_2_synthetic_data/" hreflang="ja" class="md-select__link">
              日本語
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/datascale-ai/data_engineering_book/tree/main" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../" title="Data Engineering for Large Models: Architecture, Algorithms &amp; Projects" class="md-nav__button md-logo" aria-label="Data Engineering for Large Models: Architecture, Algorithms &amp; Projects" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Data Engineering for Large Models: Architecture, Algorithms &amp; Projects
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/datascale-ai/data_engineering_book/tree/main" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Table of Contents
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2">
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 1: Infrastructure &amp; Core Concepts
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 1: Infrastructure &amp; Core Concepts
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part1/1_1_data_change/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 1: Data Revolution in the LLM Era
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part1/1_2_data_infra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 2: Data Infrastructure Selection
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3">
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 2: Text Pre-training Data Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 2: Text Pre-training Data Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part2/2_1_data_acquisition/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 3: Data Acquisition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part2/2_2_cleaning_denoising/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 4: Cleaning &amp; Deduplication
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part2/2_3_tokenization_serialization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 5: Tokenization &amp; Serialization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4">
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 3: Multimodal Data Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 3: Multimodal Data Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part3/3_1_image_text_pairs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 6: Image-Text Pair Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part3/3_2_recaptioning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 7: Recaptioning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part3/3_3_video_audio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 8: Video &amp; Audio Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 4: Alignment &amp; Synthetic Data Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 4: Alignment &amp; Synthetic Data Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_1_sft_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 9: Instruction Fine-tuning Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Chapter 10: Synthetic Data
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 10: Synthetic Data
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#chapter-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter Summary
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Chapter Summary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        Learning Objectives
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-introduction-when-data-becomes-the-bottleneck" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scenario Introduction: When Data Becomes the Bottleneck
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#101-core-concepts-and-principles-concepts-principles" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1 Core Concepts and Principles (Concepts &amp; Principles)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.1 Core Concepts and Principles (Concepts &amp; Principles)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1011-why-does-synthetic-data-quality-matter-far-more-than-quantity" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1.1 Why Does Synthetic Data Quality Matter Far More Than Quantity?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1012-textbook-level-data-textbooks-are-all-you-need" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1.2 Textbook-Level Data (Textbooks Are All You Need)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1013-code-and-math-synthesis-pot-program-of-thought" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1.3 Code and Math Synthesis: PoT (Program of Thought)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1014-multimodal-instruction-synthesis-bridging-perception" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1.4 Multimodal Instruction Synthesis: Bridging Perception
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#102-engineering-implementation-engineering-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2 Engineering Implementation (Engineering Implementation)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2 Engineering Implementation (Engineering Implementation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1021-textbook-level-data-classifier-and-synthesis-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.1 Textbook-Level Data: Classifier and Synthesis Pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1022-back-translation-deriving-questions-from-answers" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.2 Back-Translation: Deriving Questions from Answers
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1023-code-and-math-synthesis-pot-program-of-thought" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.3 Code and Math Synthesis: PoT (Program of Thought)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1024-multimodal-instruction-synthesis-llava-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.4 Multimodal Instruction Synthesis: LLaVA Pipeline
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2.4 Multimodal Instruction Synthesis: LLaVA Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-engineering-pipeline-from-pixels-to-symbols" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Engineering Pipeline: From Pixels to Symbols
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-prompt-engineering-design-and-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Prompt Engineering: Design and Considerations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1025-advanced-strategies-for-multimodal-instruction-data-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.5 Advanced Strategies for Multimodal Instruction Data Synthesis
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2.5 Advanced Strategies for Multimodal Instruction Data Synthesis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-visual-strong-model-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Visual Strong Model Distillation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-domain-specialization-mixture-of-experts-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Domain Specialization: Mixture-of-Experts Pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-evolution-instruction-generation-visual-evol-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Evolution Instruction Generation (Visual Evol-Instruct)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#103-performance-and-evaluation-performance-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.3. Performance and Evaluation (Performance &amp; Evaluation)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.3. Performance and Evaluation (Performance &amp; Evaluation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluation-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Evaluation Metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarks
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#104-pitfalls-troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.4. Pitfalls &amp; Troubleshooting
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#105-chapter-summary-and-further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.5. Chapter Summary and Further Reading
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.5. Chapter Summary and Further Reading">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_3_preference_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 11: Human Preference Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6">
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 5: Application-level Data Engineering
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 5: Application-level Data Engineering
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part5/5_1_rag_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 12: RAG Data Pipeline
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part5/5_2_mm_rag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 13: Multimodal RAG
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7">
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part 6: Capstone Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part 6: Capstone Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_1_mini_c4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 1: Building Mini-C4 Pre-training Set
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_2_legal_sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 2: Domain Expert SFT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_3_llava_instruct/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 3: Building LLaVA Multimodal Instruction Set
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_4_synthetic_textbook/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 4: Synthetic Math/Code Textbook
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../part6/6_5_mm_rag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Project 5: Multimodal RAG Financial Report Assistant
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#chapter-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Chapter Summary
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Chapter Summary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#learning-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        Learning Objectives
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-introduction-when-data-becomes-the-bottleneck" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scenario Introduction: When Data Becomes the Bottleneck
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#101-core-concepts-and-principles-concepts-principles" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1 Core Concepts and Principles (Concepts &amp; Principles)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.1 Core Concepts and Principles (Concepts &amp; Principles)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1011-why-does-synthetic-data-quality-matter-far-more-than-quantity" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1.1 Why Does Synthetic Data Quality Matter Far More Than Quantity?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1012-textbook-level-data-textbooks-are-all-you-need" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1.2 Textbook-Level Data (Textbooks Are All You Need)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1013-code-and-math-synthesis-pot-program-of-thought" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1.3 Code and Math Synthesis: PoT (Program of Thought)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1014-multimodal-instruction-synthesis-bridging-perception" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1.4 Multimodal Instruction Synthesis: Bridging Perception
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#102-engineering-implementation-engineering-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2 Engineering Implementation (Engineering Implementation)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2 Engineering Implementation (Engineering Implementation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1021-textbook-level-data-classifier-and-synthesis-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.1 Textbook-Level Data: Classifier and Synthesis Pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1022-back-translation-deriving-questions-from-answers" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.2 Back-Translation: Deriving Questions from Answers
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1023-code-and-math-synthesis-pot-program-of-thought" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.3 Code and Math Synthesis: PoT (Program of Thought)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1024-multimodal-instruction-synthesis-llava-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.4 Multimodal Instruction Synthesis: LLaVA Pipeline
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2.4 Multimodal Instruction Synthesis: LLaVA Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-engineering-pipeline-from-pixels-to-symbols" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Engineering Pipeline: From Pixels to Symbols
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-prompt-engineering-design-and-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Prompt Engineering: Design and Considerations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1025-advanced-strategies-for-multimodal-instruction-data-synthesis" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2.5 Advanced Strategies for Multimodal Instruction Data Synthesis
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.2.5 Advanced Strategies for Multimodal Instruction Data Synthesis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-visual-strong-model-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Visual Strong Model Distillation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-domain-specialization-mixture-of-experts-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Domain Specialization: Mixture-of-Experts Pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-evolution-instruction-generation-visual-evol-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Evolution Instruction Generation (Visual Evol-Instruct)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#103-performance-and-evaluation-performance-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.3. Performance and Evaluation (Performance &amp; Evaluation)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.3. Performance and Evaluation (Performance &amp; Evaluation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluation-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Evaluation Metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarks
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#104-pitfalls-troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.4. Pitfalls &amp; Troubleshooting
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#105-chapter-summary-and-further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.5. Chapter Summary and Further Reading
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10.5. Chapter Summary and Further Reading">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="chapter-10-synthetic-data-from-data-mining-to-data-farming">Chapter 10: Synthetic Data —— From "Data Mining" to "Data Farming"<a class="headerlink" href="#chapter-10-synthetic-data-from-data-mining-to-data-farming" title="Permanent link">¶</a></h1>
<h2 id="chapter-summary">Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permanent link">¶</a></h2>
<p>As the large model competition intensifies, high-quality natural data on the public internet faces depletion. We have almost "read" the entire internet, but the model's intellectual ceiling is far from reached. At this point, synthetic data is no longer optional—it is the new engine for model capability leap. This chapter deeply analyzes Microsoft's Phi series "textbook-level" data synthesis methods and explores how to transform from passive "data collectors" to active "data creators."</p>
<p>We go beyond generating text to build rigorous logical verification loops through Program of Thought (PoT) using code execution, and how to use GPT-4o and other multimodal models to synthesize complex image-text instruction data. We demonstrate how to move from simple "imitating humans" to "surpassing humans," constructing training sets that are purer and more educational than the real world through algorithmic means.</p>
<h3 id="learning-objectives">Learning Objectives<a class="headerlink" href="#learning-objectives" title="Permanent link">¶</a></h3>
<ul>
<li>Build a "Textbook Quality" classifier to filter high-value samples from massive web data.</li>
<li>Implement PoT (Program of Thought) data generation pipeline, using Python interpreter to verify correctness of math/code data.</li>
<li>Master LLaVA/GPT-4o-based multimodal instruction synthesis to construct image reasoning Q&amp;A pairs.</li>
</ul>
<h3 id="scenario-introduction-when-data-becomes-the-bottleneck">Scenario Introduction: When Data Becomes the Bottleneck<a class="headerlink" href="#scenario-introduction-when-data-becomes-the-bottleneck" title="Permanent link">¶</a></h3>
<blockquote>
<p>"You're training a small model (1.3B) specialized for Python programming. To make it as powerful as possible, you wrote crawlers to scrape all open-source code from GitHub. Yet when testing after training, you despairingly discover the model learned to write buggy code, even learning to write 'TODO: Fix this later' or 'This code is trash, do not use' inside functions.
Simply adding more data is no longer effective—the more garbage you feed the model, the more chaotic its output. Then Microsoft's Phi-1 paper hits you like a wake-up call: 'Textbooks Are All You Need.' What you need is textbook-like code with clear logic, perfect comments, and progressive teaching—not 'spaghetti code' only God and the original author can understand. But where do you find hundreds of billions of tokens of perfect textbooks? Since we can't find them, we must learn to 'create' this data out of thin air. How to build a tireless 'virtual professor' to batch-produce these perfect textbooks? This is the core engineering challenge this chapter addresses."</p>
</blockquote>
<hr>
<h2 id="101-core-concepts-and-principles-concepts-principles">10.1 Core Concepts and Principles (Concepts &amp; Principles)<a class="headerlink" href="#101-core-concepts-and-principles-concepts-principles" title="Permanent link">¶</a></h2>
<p>The core challenge of synthetic data lies in <strong>quality control</strong> and <strong>verification loops</strong>. Because model-generated text often contains hallucinations or errors without checking. Training models on erroneous data leads to "Model Autophagy" or "Model Collapse"—where model output variance gradually vanishes and content becomes extremely homogeneous and detached from reality. The three methods introduced in this chapter address quality issues for text, code/math, and multimodal data respectively.</p>
<h3 id="1011-why-does-synthetic-data-quality-matter-far-more-than-quantity">10.1.1 Why Does Synthetic Data Quality Matter Far More Than Quantity?<a class="headerlink" href="#1011-why-does-synthetic-data-quality-matter-far-more-than-quantity" title="Permanent link">¶</a></h3>
<p>In early deep learning, we believed "data volume equals justice"—that with enough data, models could learn everything. But in the synthetic data era, this dogma has been overturned.</p>
<p><strong>Signal-to-Noise Ratio Theory:</strong>
Model training is essentially an information compression process. High-quality data (e.g., textbooks) has extremely high information density and rigorous logical chains—the model needs few samples to capture underlying patterns. Low-quality data (e.g., forum spam, chit-chat) is full of noise and logical gaps. If training sets mix large amounts of low-quality synthetic data, the model will be forced to fit this noise to reduce Loss, causing "logic circuits" to short-circuit.</p>
<p>Learning physics: reading a classic like <em>Feynman Lectures</em> (high-quality synthetic data) surpasses watching 10,000 fragmented physics popularization videos (low-quality data). Phi-1's success proved: 6B tokens of textbook-level data can outperform 1000B tokens of web-crawled data in training effect. In synthetic data, <strong>verification cost</strong> has become the new currency.</p>
<h3 id="1012-textbook-level-data-textbooks-are-all-you-need">10.1.2 Textbook-Level Data (Textbooks Are All You Need)<a class="headerlink" href="#1012-textbook-level-data-textbooks-are-all-you-need" title="Permanent link">¶</a></h3>
<p>Microsoft Phi-1's core idea: rather than training with 1TB of garbage data, use 6B tokens of high-quality data. Its core lies in building a "filter" and an "amplifier."</p>
<p>First, we don't completely abandon web data—we train a classifier (Quality Classifier) to identify content with "educational value." This isn't just checking grammar but whether content is logically self-consistent and contains definitions and reasoning. Second, we use powerful generative models (e.g., GPT-4) as "amplifiers," synthesizing self-contained knowledge fragments with similar style but entirely new content based on these high-quality snippets.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../../../images/part4/%E5%9B%BE10_1_Phi-1%E6%B5%81%E7%A8%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png" data-desc-position="bottom"><img alt="Figure 10-1: Phi-1 Process Diagram" src="../../../images/part4/%E5%9B%BE10_1_Phi-1%E6%B5%81%E7%A8%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png"></a>
<em>Figure 10-1: Phi-1 Process Diagram</em></p>
<h3 id="1013-code-and-math-synthesis-pot-program-of-thought">10.1.3 Code and Math Synthesis: PoT (Program of Thought)<a class="headerlink" href="#1013-code-and-math-synthesis-pot-program-of-thought" title="Permanent link">¶</a></h3>
<p>LLMs are essentially probabilistic models—they don't have true logic inference chips. Therefore, when LLMs perform arithmetic (e.g., 234 * 567) or complex logical derivation, they easily hallucinate. PoT (Program of Thought) thinks: since LLMs aren't good at calculation but good at translation, have them "translate" math problems into code, then let the Python interpreter compute the result.</p>
<p>This is the only domain in synthetic data achieving <strong>100% accuracy verification</strong>. We put generated code into a Python sandbox for execution. If it errors, discard; if it runs successfully, the execution result is Ground Truth. This "execution equals verification" mechanism completely solves the synthetic data verifiability problem, enabling low-cost generation of infinite math and logical reasoning data.</p>
<p><strong>Table 10-1: Synthetic Data Verification Strategy Comparison</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data Type</th>
<th style="text-align: left;">Generator</th>
<th style="text-align: left;">Core Challenge</th>
<th style="text-align: left;">Verifier</th>
<th style="text-align: left;">Verification Mechanism</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>General Text</strong></td>
<td style="text-align: left;">GPT-4 / Gemini</td>
<td style="text-align: left;">Hallucination</td>
<td style="text-align: left;">LLM (Judge) / Reward Model</td>
<td style="text-align: left;">Depends on strong model scoring; low consistency; prone to "judge bias"</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Math/Logic</strong></td>
<td style="text-align: left;">GPT-4 + PoT Prompt</td>
<td style="text-align: left;">Calculation errors</td>
<td style="text-align: left;">Python Interpreter</td>
<td style="text-align: left;"><strong>Execution consistency</strong>: Code run result matches expected answer; logic absolutely correct</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Code</strong></td>
<td style="text-align: left;">DeepSeek Coder / GPT-4</td>
<td style="text-align: left;">Syntax errors, logic bugs</td>
<td style="text-align: left;">Unit Tests / Compiler</td>
<td style="text-align: left;"><strong>Unit tests</strong>: Via assert or successful compilation, ensure functionality</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Multimodal</strong></td>
<td style="text-align: left;">GPT-4o / LLaVA</td>
<td style="text-align: left;">Visual hallucination (fabrication)</td>
<td style="text-align: left;">CLIP Score / Grounding DINO</td>
<td style="text-align: left;">Check if generated description matches image Embedding; prevent fabricating non-existent objects</td>
</tr>
</tbody>
</table>
<h3 id="1014-multimodal-instruction-synthesis-bridging-perception">10.1.4 Multimodal Instruction Synthesis: Bridging Perception<a class="headerlink" href="#1014-multimodal-instruction-synthesis-bridging-perception" title="Permanent link">¶</a></h3>
<p>For image data, traditional annotation costs are extremely high and descriptions are brief. LLaVA proposed a brilliant "blind men and elephant" strategy. We use existing detection models to convert images into symbolic text descriptions (Caption + Bounding Boxes), then feed this pure text to strong text-only models (e.g., GPT-4). GPT-4 can't see images, but it can "imagine" image content through this metadata and generate complex reasoning dialogue based on it. This not only solves multimodal data scarcity but greatly elevates instruction complexity and logic.</p>
<hr>
<h2 id="102-engineering-implementation-engineering-implementation">10.2 Engineering Implementation (Engineering Implementation)<a class="headerlink" href="#102-engineering-implementation-engineering-implementation" title="Permanent link">¶</a></h2>
<p>This section delves into code-level implementation, building a complete data synthesis pipeline. We focus on ensuring data diversity and preventing generated "textbooks" from being monotonous.</p>
<h3 id="1021-textbook-level-data-classifier-and-synthesis-pipeline">10.2.1 Textbook-Level Data: Classifier and Synthesis Pipeline<a class="headerlink" href="#1021-textbook-level-data-classifier-and-synthesis-pipeline" title="Permanent link">¶</a></h3>
<p><strong>Step 1: Train Quality Classifier</strong>
We need to train a lightweight model (e.g., Random Forest or BERT-Tiny) to select "textbooks" from massive data. The key here is annotation data source. Typically we need expert or GPT-4 careful annotation of thousands of samples as seed.</p>
<p><strong>Code Implementation: Feature Engineering and Training</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="c1"># 1. Prepare annotation data</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="c1"># First annotate a small number of samples (e.g., 1000) with GPT-4 as "gold standard"</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="c1"># Prompt: "Determine if this text is of educational value for a student..."</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="c1"># Label 1: High Quality (Textbook-like), Label 0: Low Quality (Noise)</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="c1"># This step is critical; annotation quality directly determines classifier ceiling</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    <span class="p">{</span><span class="s2">"text"</span><span class="p">:</span> <span class="s2">"Python lists are mutable sequences..."</span><span class="p">,</span> <span class="s2">"label"</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    <span class="p">{</span><span class="s2">"text"</span><span class="p">:</span> <span class="s2">"Hey guys check out my cat photo..."</span><span class="p">,</span> <span class="s2">"label"</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    <span class="c1"># ... more data</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="p">]</span> 
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a><span class="c1"># 2. Build classifier pipeline</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="c1"># Phi-1 paper uses pretrained model Embedding; here simplified to TF-IDF for demonstration</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="c1"># In production, recommend DeBERTa-v3-small or similar lightweight Transformer for Embedding</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    <span class="p">(</span><span class="s1">'tfidf'</span><span class="p">,</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="s1">'english'</span><span class="p">)),</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    <span class="p">(</span><span class="s1">'clf'</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a><span class="p">])</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a><span class="c1"># 3. Train</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'text'</span><span class="p">]</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a><span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a><span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a><span class="c1"># 4. Predict (Filtering Phase)</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a><span class="c1"># Score massive web data; retain only high-scoring data</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a><span class="n">web_snippet</span> <span class="o">=</span> <span class="s2">"Standard library documentation for Python..."</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a><span class="n">score</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="n">web_snippet</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a><span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">"Keep this data for training: High Educational Value"</span><span class="p">)</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a><span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">"Discard: Low Signal-to-Noise Ratio"</span><span class="p">)</span>
</code></pre></div><p></p>
<p><strong>Step 2: Synthetic Textbook Fragment Generation</strong>
With classifier-filtered seed data, we need to "expand" it. Prompt design here requires extreme skill.</p>
<p>Prompt iteration: Synthetic Python tutorial
* <strong>V1 Prompt:</strong> "Write a tutorial about Python lists."
    * <strong>Result:</strong> Flat narrative, lacking depth, like mediocre blog post.
* <strong>V3 Prompt (Phi-style):</strong> Introduced specific pedagogy, requiring definitions, comparisons, complexity analysis, and pitfall warnings.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="c1"># V3 Prompt - Textbook-style synthesis</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="n">synthetic_textbook_prompt</span> <span class="o">=</span> <span class="s2">"""</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="s2">### ROLE</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="s2">You are a professor of Computer Science writing a definitive textbook on Python.</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="s2">### OBJECTIVE</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="s2">Write a comprehensive, self-contained chapter section on the topic: "List Comprehensions vs. Map/Filter".</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span class="s2">### REQUIREMENTS</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span class="s2">1. **Tone**: Educational, clear, precise, and rigorous. Avoid conversational filler.</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="s2">2. **Structure**:</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span class="s2">   - Start with a conceptual definition explaining *why* this feature exists.</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a><span class="s2">   - Provide a "Before and After" code example (Loop vs. Comprehension).</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a><span class="s2">   - Explain the *computational complexity* (Big O) implications.</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a><span class="s2">   - Include a "Common Pitfall" section (e.g., readability vs. brevity).</span>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a><span class="s2">3. **Diversity**: Use realistic variable names (e.g., `inventory_items`, `sensor_readings`), NOT generic ones like `x`, `y`, `foo`. </span>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a><span class="s2">   (This is crucial to prevent the model from overfitting to toy examples).</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a><span class="s2">### OUTPUT</span>
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span class="s2">[Markdown Content]</span>
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a><span class="s2">"""</span>
</code></pre></div>
<h3 id="1022-back-translation-deriving-questions-from-answers">10.2.2 Back-Translation: Deriving Questions from Answers<a class="headerlink" href="#1022-back-translation-deriving-questions-from-answers" title="Permanent link">¶</a></h3>
<p>Besides generating from scratch, another efficient method is "back-translation." In code, we often easily find high-quality code snippets (e.g., high-star library functions on GitHub) but lack corresponding natural language instructions.</p>
<p>We can use LLM summarization: input complex code, ask the model: "Please write as detailed as possible various user requirement instructions such that this code exactly solves the problem." This quickly generates massive (Instruction, Output) pairs with guaranteed high-quality human code. This method is especially suitable for enhancing model understanding of complex code logic.</p>
<h3 id="1023-code-and-math-synthesis-pot-program-of-thought">10.2.3 Code and Math Synthesis: PoT (Program of Thought)<a class="headerlink" href="#1023-code-and-math-synthesis-pot-program-of-thought" title="Permanent link">¶</a></h3>
<p>This is the strongest means to ensure synthetic data correctness. By forcing the model to generate code, we convert fuzzy natural language reasoning into precise program logic.</p>
<p><strong>Core Code Breakdown: Generation and Verification Loop</strong>
</p><div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="c1"># 1. PoT Generation Prompt</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="c1"># Require model to write solution steps as Python function solver()</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="n">pot_prompt</span> <span class="o">=</span> <span class="s2">"""</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span class="s2">Question: Janet has 3 times as many eggs as Bob. Bob has 5 eggs. How many eggs do they have in total?</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="s2">Instruction:</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a><span class="s2">Write a Python function named `solver()` that returns the answer.</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="s2">Do not output the number directly. Write the code to calculate it.</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a><span class="s2">Include comments explaining the logic.</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a><span class="s2">"""</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a><span class="c1"># Assume LLM returns the following code string</span>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a><span class="n">generated_code</span> <span class="o">=</span> <span class="s2">"""</span>
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a><span class="s2">def solver():</span>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a><span class="s2">    # Bob has 5 eggs</span>
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a><span class="s2">    bob_eggs = 5</span>
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a><span class="s2">    # Janet has 3 times as many as Bob</span>
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a><span class="s2">    janet_eggs = 3 * bob_eggs</span>
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#__codelineno-2-23"></a><span class="s2">    # Total eggs</span>
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#__codelineno-2-24"></a><span class="s2">    total = janet_eggs + bob_eggs</span>
<a id="__codelineno-2-25" name="__codelineno-2-25" href="#__codelineno-2-25"></a><span class="s2">    return total</span>
<a id="__codelineno-2-26" name="__codelineno-2-26" href="#__codelineno-2-26"></a><span class="s2">"""</span>
<a id="__codelineno-2-27" name="__codelineno-2-27" href="#__codelineno-2-27"></a>
<a id="__codelineno-2-28" name="__codelineno-2-28" href="#__codelineno-2-28"></a><span class="c1"># 2. Code Execution Sandbox</span>
<a id="__codelineno-2-29" name="__codelineno-2-29" href="#__codelineno-2-29"></a><span class="c1"># WARNING: Directly executing generated code is extremely dangerous; must run in sandbox</span>
<a id="__codelineno-2-30" name="__codelineno-2-30" href="#__codelineno-2-30"></a><span class="k">def</span><span class="w"> </span><span class="nf">execute_generated_code</span><span class="p">(</span><span class="n">code_str</span><span class="p">):</span>
<a id="__codelineno-2-31" name="__codelineno-2-31" href="#__codelineno-2-31"></a>    <span class="k">try</span><span class="p">:</span>
<a id="__codelineno-2-32" name="__codelineno-2-32" href="#__codelineno-2-32"></a>        <span class="c1"># In production use Docker, gVisor, or nsjail for isolation</span>
<a id="__codelineno-2-33" name="__codelineno-2-33" href="#__codelineno-2-33"></a>        <span class="n">local_scope</span> <span class="o">=</span> <span class="p">{}</span>
<a id="__codelineno-2-34" name="__codelineno-2-34" href="#__codelineno-2-34"></a>
<a id="__codelineno-2-35" name="__codelineno-2-35" href="#__codelineno-2-35"></a>        <span class="c1"># Limit execution time to prevent infinite loops</span>
<a id="__codelineno-2-36" name="__codelineno-2-36" href="#__codelineno-2-36"></a>        <span class="c1"># Here uses simplified exec for demo; production needs resource module for CPU/memory limits</span>
<a id="__codelineno-2-37" name="__codelineno-2-37" href="#__codelineno-2-37"></a>        <span class="n">exec</span><span class="p">(</span><span class="n">code_str</span><span class="p">,</span> <span class="p">{},</span> <span class="n">local_scope</span><span class="p">)</span>
<a id="__codelineno-2-38" name="__codelineno-2-38" href="#__codelineno-2-38"></a>
<a id="__codelineno-2-39" name="__codelineno-2-39" href="#__codelineno-2-39"></a>        <span class="k">if</span> <span class="s1">'solver'</span> <span class="ow">in</span> <span class="n">local_scope</span><span class="p">:</span>
<a id="__codelineno-2-40" name="__codelineno-2-40" href="#__codelineno-2-40"></a>            <span class="n">result</span> <span class="o">=</span> <span class="n">local_scope</span><span class="p">[</span><span class="s1">'solver'</span><span class="p">]()</span>
<a id="__codelineno-2-41" name="__codelineno-2-41" href="#__codelineno-2-41"></a>            <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="s2">"Success"</span>
<a id="__codelineno-2-42" name="__codelineno-2-42" href="#__codelineno-2-42"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-2-43" name="__codelineno-2-43" href="#__codelineno-2-43"></a>            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"No solver function found"</span>
<a id="__codelineno-2-44" name="__codelineno-2-44" href="#__codelineno-2-44"></a>    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<a id="__codelineno-2-45" name="__codelineno-2-45" href="#__codelineno-2-45"></a>        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="sa">f</span><span class="s2">"Execution Error: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
<a id="__codelineno-2-46" name="__codelineno-2-46" href="#__codelineno-2-46"></a>
<a id="__codelineno-2-47" name="__codelineno-2-47" href="#__codelineno-2-47"></a><span class="c1"># 3. Verification and Data Saving</span>
<a id="__codelineno-2-48" name="__codelineno-2-48" href="#__codelineno-2-48"></a><span class="n">result</span><span class="p">,</span> <span class="n">status</span> <span class="o">=</span> <span class="n">execute_generated_code</span><span class="p">(</span><span class="n">generated_code</span><span class="p">)</span>
<a id="__codelineno-2-49" name="__codelineno-2-49" href="#__codelineno-2-49"></a>
<a id="__codelineno-2-50" name="__codelineno-2-50" href="#__codelineno-2-50"></a><span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="s2">"Success"</span><span class="p">:</span>
<a id="__codelineno-2-51" name="__codelineno-2-51" href="#__codelineno-2-51"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Verified Answer: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<a id="__codelineno-2-52" name="__codelineno-2-52" href="#__codelineno-2-52"></a>    <span class="c1"># Data saving strategy:</span>
<a id="__codelineno-2-53" name="__codelineno-2-53" href="#__codelineno-2-53"></a>    <span class="c1"># Strategy A (PoT): Save Instruction -&gt; Code. Train model to write code for solving.</span>
<a id="__codelineno-2-54" name="__codelineno-2-54" href="#__codelineno-2-54"></a>    <span class="c1"># Strategy B (CoT): Save Instruction -&gt; "Let's calculate... [Reasoning]... The answer is {result}".</span>
<a id="__codelineno-2-55" name="__codelineno-2-55" href="#__codelineno-2-55"></a>    <span class="c1"># Strategy B uses code as intermediate step to generate pure text reasoning data.</span>
<a id="__codelineno-2-56" name="__codelineno-2-56" href="#__codelineno-2-56"></a>    <span class="n">save_to_dataset</span><span class="p">(</span><span class="n">pot_prompt</span><span class="p">,</span> <span class="n">generated_code</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<a id="__codelineno-2-57" name="__codelineno-2-57" href="#__codelineno-2-57"></a><span class="k">else</span><span class="p">:</span>
<a id="__codelineno-2-58" name="__codelineno-2-58" href="#__codelineno-2-58"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">"Discard bad data: Code failed to execute"</span><span class="p">)</span>
</code></pre></div>
<strong>Pro Tip:</strong> Generated data can train not only PoT but also ordinary CoT models. Method: use successfully executed code as "intermediate step," execution result as "final answer," reversely construct <code>&lt;thinking&gt;...code...&lt;/thinking&gt;&lt;answer&gt;...result...&lt;/answer&gt;</code> format. This "borrowing chickens to lay eggs" method can significantly improve pure text model arithmetic accuracy.<p></p>
<h3 id="1024-multimodal-instruction-synthesis-llava-pipeline">10.2.4 Multimodal Instruction Synthesis: LLaVA Pipeline<a class="headerlink" href="#1024-multimodal-instruction-synthesis-llava-pipeline" title="Permanent link">¶</a></h3>
<p>Using text-only models to synthesize multimodal data is LLaVA's innovation. This method's core lies in <strong>symbolizing</strong> visual information—since text-only models (e.g., GPT-4) can't see images, we translate images into "code" they can read.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../../../images/part4/%E5%9B%BE10_2_LLaVA%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90%E6%B5%81%E7%A8%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png" data-desc-position="bottom"><img alt="Figure 10-2: LLaVA Data Synthesis Process Diagram" src="../../../images/part4/%E5%9B%BE10_2_LLaVA%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90%E6%B5%81%E7%A8%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png"></a>
<em>Figure 10-2: LLaVA Data Synthesis Process Diagram</em></p>
<h4 id="1-engineering-pipeline-from-pixels-to-symbols">1. Engineering Pipeline: From Pixels to Symbols<a class="headerlink" href="#1-engineering-pipeline-from-pixels-to-symbols" title="Permanent link">¶</a></h4>
<p>Before Prompt design, we need toolchain to "deconstruct" images into structured data (Metadata) readable by text models:</p>
<ol>
<li>
<p><strong>Global Semantics (Captioning)</strong></p>
<ul>
<li><strong>Tool</strong>: CLIP or BLIP for one-sentence description.</li>
<li><strong>Role</strong>: Provide overall context.</li>
<li><strong>Output example</strong>: <code>"A young girl riding a horse on a beach at sunset."</code></li>
</ul>
</li>
<li>
<p><strong>Local Details (Object Detection)</strong></p>
<ul>
<li><strong>Tool</strong>: Grounding DINO to extract objects and coordinates (Bounding Box).</li>
<li><strong>Role</strong>: Provide spatial anchoring entities.</li>
<li><strong>Output example</strong>: <code>{'girl': [100, 200, 300, 400], ...}</code></li>
</ul>
</li>
<li>
<p><strong>Data Synthesis</strong></p>
<ul>
<li><strong>Action</strong>: Fill above information into Prompt, call GPT-4 to generate dialogue.</li>
</ul>
</li>
</ol>
<h4 id="2-prompt-engineering-design-and-considerations">2. Prompt Engineering: Design and Considerations<a class="headerlink" href="#2-prompt-engineering-design-and-considerations" title="Permanent link">¶</a></h4>
<p>With structured data, Prompt design becomes the key to data quality. Below is LLaVA-style Prompt template and architectural considerations:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="c1"># System Prompt for Multimodal Data Generation</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="n">multimodal_gen_prompt</span> <span class="o">=</span> <span class="s2">"""</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="s2">### CONTEXT</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="s2">You are an AI visual assistant. You cannot see the image directly, but I will provide its metadata.</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="s2">Your task is to generate a conversation between a Human and Yourself about this image.</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="s2">### IMAGE METADATA</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="s2"># [Data injection point]: Fill pipeline-extracted data here</span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a><span class="s2">- **Caption**: "</span><span class="si">{caption}</span><span class="s2">"</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a><span class="s2">- **Objects**: </span><span class="si">{object_list_with_boxes}</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a><span class="s2">### INSTRUCTIONS</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a><span class="s2">1. **Conversation Style**: Generate a multi-turn Q&amp;A (User asking, Assistant answering).</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a><span class="s2">2. **Reasoning**: The Human should ask complex questions (e.g., "What suggests this is a safe environment?"). You answer based on the visual evidence.</span>
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a><span class="s2">3. **Spatial Awareness**: Use the bounding box info to describe relative positions if asked (e.g., "The ocean is in the background...").</span>
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a><span class="s2">4. **Visual Consistency**: Do NOT hallucinate objects not listed in the metadata.</span>
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a><span class="s2">"""</span>
</code></pre></div>
<p><strong>Architectural considerations behind Prompt design:</strong></p>
<ul>
<li>
<p><strong>Why provide both Caption and Objects? (Complementarity)</strong>
    Objects alone (girl, horse, ocean) are discrete, lacking action and atmosphere; Caption alone (girl riding horse) lacks specific location. Combined, GPT-4 can construct complete mental "scene graph."</p>
</li>
<li>
<p><strong>Why emphasize "Spatial Awareness"? (Spatial alignment)</strong>
    Text models inherently lack spatial sense. By forcing them to process <code>[x1, y1, x2, y2]</code> coordinate data, we're forcing the text model to learn "visual alignment"—understanding pixel regions corresponding to "left," "lower right," etc.</p>
</li>
<li>
<p><strong>Why add "Visual Consistency" constraint? (Hallucination suppression)</strong>
    Text models' biggest flaw is easy "brain-filling." E.g., seeing "beach" they might fabricate "seagulls flying." Must explicitly prohibit generating objects not in Metadata to ensure high signal-to-noise ratio.</p>
</li>
<li>
<p><strong>Why generate "Complex Reasoning"? (Data dimension upgrade)</strong>
    Simple "What's this? A horse" has no training value. We need to leverage GPT-4's intelligence to artificially create "requires thinking" samples (e.g., causal inference, sentiment analysis) through synthesis, so small models (Student) can distill large model reasoning through learning.</p>
</li>
</ul>
<hr>
<h3 id="1025-advanced-strategies-for-multimodal-instruction-data-synthesis">10.2.5 Advanced Strategies for Multimodal Instruction Data Synthesis<a class="headerlink" href="#1025-advanced-strategies-for-multimodal-instruction-data-synthesis" title="Permanent link">¶</a></h3>
<p>Although "symbolic reasoning" based on text-only models (like early LLaVA v1) pioneered multimodal instruction synthesis, its main defect is "lossy compression of visual information"—text models cannot directly perceive pixel-level visual features; relying only on metadata for reasoning easily causes hallucination.</p>
<p>To break this bottleneck, industry and academia have evolved three more mainstream and efficient synthesis strategies: <strong>Visual Strong Model Distillation</strong>, <strong>Mixture-of-Experts Pipeline</strong>, and <strong>Evolution Instruction Generation</strong>.</p>
<h4 id="1-visual-strong-model-distillation">1. Visual Strong Model Distillation<a class="headerlink" href="#1-visual-strong-model-distillation" title="Permanent link">¶</a></h4>
<p>This is currently (as of 2024-2025) the most mainstream method for building high-performance open-source multimodal models (e.g., LLaVA-NeXT, ShareGPT4V), often considered SOTA (State-of-the-Art).</p>
<p><strong>Core Idea</strong>
This method abandons "using text models to guess visual content" and adopts the "Teacher-Student" distillation paradigm. Using closed-source top multimodal models (e.g., GPT-4o, Gemini 1.5 Pro) as "teacher models," directly process raw image signals to generate high-quality, high-density detailed descriptions (Dense Caption) and complex reasoning Q&amp;A pairs for open-source models (student models) to learn.</p>
<p><strong>Advantage Analysis</strong>
* <strong>Eliminate modality gap</strong>: Teacher model directly "sees" image pixels, capturing lighting, texture, micro-expressions that text metadata cannot convey.
* <strong>Suppress hallucination</strong>: Descriptions based on real visual input greatly reduce factual error probability.</p>
<p><strong>Implementation Flow</strong>
The core lies in building a Prompt that can induce teacher model to output exhaustive information. Below is standard distillation flow pseudocode:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">generate_dense_instruction</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">api_client</span><span class="p">):</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="w">    </span><span class="sd">"""</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="sd">    Use SOTA MLLM to generate high-density multimodal instruction data</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="sd">    """</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>    <span class="c1"># System Prompt key: Require extremely detailed capture and logical association</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>    <span class="n">distillation_prompt</span> <span class="o">=</span> <span class="s2">"""</span>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="s2">    You are an expert visual analyst. Analyze the provided image with extreme detail.</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a><span class="s2">    Tasks:</span>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a><span class="s2">    1. Dense Captioning: Provide a comprehensive description of every corner of the image, covering colors, textures, lighting, and background details.</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="s2">    2. Object Relationships: Analyze the interactions between objects (e.g., causality, spatial relations).</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a><span class="s2">    3. OCR Extraction: Transcribe any visible text verbatim.</span>
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a><span class="s2">    4. Q&amp;A Generation: Based on the visual details above, create a logical reasoning question that cannot be answered without looking at the image.</span>
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a><span class="s2">    """</span>
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a>
<a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a>    <span class="c1"># Key difference: Input contains real Image Tensor, not merely Bounding Box</span>
<a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a>    <span class="n">response</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
<a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a>        <span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o"</span><span class="p">,</span>
<a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a>        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
<a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a>            <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">distillation_prompt</span><span class="p">},</span>
<a id="__codelineno-4-22" name="__codelineno-4-22" href="#__codelineno-4-22"></a>            <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="p">[{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"image_url"</span><span class="p">,</span> <span class="s2">"url"</span><span class="p">:</span> <span class="n">image_path</span><span class="p">}]}</span>
<a id="__codelineno-4-23" name="__codelineno-4-23" href="#__codelineno-4-23"></a>        <span class="p">]</span>
<a id="__codelineno-4-24" name="__codelineno-4-24" href="#__codelineno-4-24"></a>    <span class="p">)</span>
<a id="__codelineno-4-25" name="__codelineno-4-25" href="#__codelineno-4-25"></a>
<a id="__codelineno-4-26" name="__codelineno-4-26" href="#__codelineno-4-26"></a>    <span class="k">return</span> <span class="n">parse_response</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>
<hr>
<h4 id="2-domain-specialization-mixture-of-experts-pipeline">2. Domain Specialization: Mixture-of-Experts Pipeline<a class="headerlink" href="#2-domain-specialization-mixture-of-experts-pipeline" title="Permanent link">¶</a></h4>
<p>For vertical domains where general models struggle—Document AI, complex chart analysis, or autonomous driving data—general visual distillation often lacks precision. Adopting "Mixture-of-Experts" strategy is best practice.</p>
<p><strong>Core Logic</strong>
This method doesn't rely on single model's end-to-end capability but assembles multiple specialized small models (Experts) as perception frontend, converting unstructured images into fine structured data for LLM integration.</p>
<p>Logic flow:
$<span class="arithmatex">\(\text{Image} \xrightarrow{\text{Experts}} [\text{OCR} + \text{Layout} + \text{Detection}] \xrightarrow{\text{Aggregation}} \text{Structured Context} \xrightarrow{\text{LLM}} \text{Instruction}\)</span>$</p>
<p><strong>Application Scenarios</strong>
Typical applications include financial invoice processing, medical imaging reports (DICOM), etc.</p>
<ol>
<li><strong>OCR Expert (e.g., PaddleOCR)</strong>: Extract all text and precise coordinates <span class="arithmatex">\((x_1, y_1, x_2, y_2)\)</span> from image.</li>
<li><strong>Layout Expert (e.g., LayoutLM)</strong>: Parse document topology, identify table rows/columns, paragraph hierarchy, and title relationships.</li>
<li><strong>Synthesis (LLM)</strong>: Fill structured data into Prompt template.<ul>
<li><em>Example Prompt:</em> "This is an invoice's structured data, invoice number at (100, 200), total amount 500.00. Please generate a multi-turn Q&amp;A about 'financial audit verification' based on this."</li>
</ul>
</li>
</ol>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../../../images/part4/%E5%9B%BE10_3_%E5%A4%9A%E4%B8%93%E5%AE%B6%E6%B7%B7%E5%90%88%E6%B5%81%E6%B0%B4%E7%BA%BF%E7%A4%BA%E6%84%8F%E5%9B%BE.png" data-desc-position="bottom"><img alt="Figure 10-3: Mixture-of-Experts Pipeline Diagram" src="../../../images/part4/%E5%9B%BE10_3_%E5%A4%9A%E4%B8%93%E5%AE%B6%E6%B7%B7%E5%90%88%E6%B5%81%E6%B0%B4%E7%BA%BF%E7%A4%BA%E6%84%8F%E5%9B%BE.png"></a>
<em>Figure 10-3: Mixture-of-Experts Pipeline Diagram</em></p>
<hr>
<h4 id="3-evolution-instruction-generation-visual-evol-instruct">3. Evolution Instruction Generation (Visual Evol-Instruct)<a class="headerlink" href="#3-evolution-instruction-generation-visual-evol-instruct" title="Permanent link">¶</a></h4>
<p>Inspired by WizardLM in text domain, Visual Evol-Instruct aims to solve training data "homogenization" and "oversimplification." When base dataset only contains simple recognition tasks (e.g., "What's in the image?"), models cannot learn higher-order reasoning. This method forces "dimensional upgrade" of existing data through Prompt Engineering.</p>
<p><strong>Core Logic</strong>
$<span class="arithmatex">\(\text{Simple VQA} \xrightarrow{\text{Complexity Constraints}} \text{Complex Reasoning VQA}\)</span>$</p>
<p>By applying specific evolution instructions to LLM, data complexity can be elevated in these dimensions:</p>
<ul>
<li><strong>Reasoning Deepening</strong>:<ul>
<li><em>Original</em>: "What is this person holding?"</li>
<li><em>Evolved</em>: "Based on the object's use and the person's clothing, infer this person's profession and what activities they might perform next."</li>
</ul>
</li>
<li><strong>Counterfactual Reasoning</strong>:<ul>
<li><em>Original</em>: "The car in the image is red."</li>
<li><em>Evolved</em>: "If the red sports car in the image were replaced with an old bicycle, how would the scene's atmosphere change? Does this fit the modern architecture style in the background?"</li>
</ul>
</li>
<li><strong>Comparative Analysis</strong>:<ul>
<li>Input two similar images, require model to analyze subtle differences (lighting changes, object displacement), training model's fine-grained observation.</li>
</ul>
</li>
</ul>
<p>Through combined use of these three strategies, we can build high-quality multimodal instruction datasets rich in visual detail and deep logical reasoning, laying solid foundation for training LLaVA, MiniGPT-4, and similar models.</p>
<h2 id="103-performance-and-evaluation-performance-evaluation">10.3. Performance and Evaluation (Performance &amp; Evaluation)<a class="headerlink" href="#103-performance-and-evaluation-performance-evaluation" title="Permanent link">¶</a></h2>
<p>After training on synthetic data, evaluation is especially important—we need to confirm whether the model truly "learned" or merely "memorized" patterns in synthetic data.</p>
<h3 id="evaluation-metrics">Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Pass@1 (Code):</strong> For PoT synthetic data, we test Pass@1 on HumanEval. Phi-1 achieved 50%+ Pass@1 with only 6B data, surpassing many models trained on 100x more data. This proves data quality's overwhelming advantage.</li>
<li><strong>Hallucination Rate:</strong> Compare synthetic multimodal answers with original image CLIP similarity to detect whether non-existent objects were generated. We can build negative sample sets specifically inducing model to answer non-existent objects, checking if model refuses.</li>
<li><strong>Decontamination:</strong> A dirty but necessary step. Check whether synthetic data inadvertently contains test set (e.g., HumanEval) questions. Through N-gram overlap detection, ensure model generalizes rather than cheats.</li>
</ul>
<h3 id="benchmarks">Benchmarks<a class="headerlink" href="#benchmarks" title="Permanent link">¶</a></h3>
<ul>
<li><strong>PoT vs CoT:</strong> On math (e.g., GSM8K), PoT typically outperforms pure text CoT by 5-10%. Reason: PoT outsources calculation to CPU (best at computation) while GPU focuses on logic translation—optimal compute allocation.</li>
</ul>
<hr>
<h2 id="104-pitfalls-troubleshooting">10.4. Pitfalls &amp; Troubleshooting<a class="headerlink" href="#104-pitfalls-troubleshooting" title="Permanent link">¶</a></h2>
<p>Synthetic data is beautiful but full of traps. A slight slip and the model falls into "self-congratulation" loops.</p>
<ul>
<li>
<p><strong>Pitfall 1: Self-Confirmation Bias</strong></p>
<ul>
<li><strong>Symptom:</strong> Model-generated code runs but logic is wrong (e.g., 2+2=5, and model-generated test cases are also wrong, coincidentally passing the wrong function).</li>
<li><strong>Fix:</strong> Must introduce external, deterministic Solver or human-reviewed Unit Test library. Never fully rely on model-generated test cases to verify model-generated code—it's like letting the criminal judge themselves.</li>
</ul>
</li>
<li>
<p><strong>Pitfall 2: Lack of Visual Grounding</strong></p>
<ul>
<li><strong>Symptom:</strong> In multimodal synthetic data, model discusses details not in metadata (fabrication). E.g., Metadata only has "dog," but model describes "dog collar color" when the dog in the image has no collar.</li>
<li><strong>Fix:</strong> Add strict instruction in Prompt: "Only strictly rely on the provided metadata. Do not invent details." Also use CLIP Score to filter out generated text with too low similarity to original image.</li>
</ul>
</li>
<li>
<p><strong>Pitfall 3: The Homogenization Trap</strong></p>
<ul>
<li><strong>Symptom:</strong> If all data is GPT-4 generated, your model becomes a "low-end GPT-4," losing diversity. All answer tones and sentence structures are strikingly consistent.</li>
<li><strong>Fix:</strong> <strong>Entropy Injection</strong>. Randomly inject different Personas in Prompt (e.g., "grumpy programmer," "patient kindergarten teacher"), or require different programming styles (recursive vs. iterative), forcing data distribution expansion.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="105-chapter-summary-and-further-reading">10.5. Chapter Summary and Further Reading<a class="headerlink" href="#105-chapter-summary-and-further-reading" title="Permanent link">¶</a></h2>
<p>The "Textbooks Are All You Need" paradigm established the core principle that data quality (educational value) takes priority over quantity. Synthetic data technology grants us the ability to precisely control data signal-to-noise ratio. Under this framework, Program of Thought (PoT) guarantees data rigor by converting reasoning into executable code and using compiler determinism for verification. Meanwhile, Symbolic-to-Synthetic method uses metadata (e.g., Bounding Box) to guide text models to generate multimodal content, achieving effective conversion from unimodal to multimodal data. This evolution marks data engineering's transition from passive "mining" to active "production": through seed prompt construction, instruction complexity (Evolution), quality filtering, and final synthesis, systematically building high-quality datasets via standard industrial process.</p>
<h3 id="references">References<a class="headerlink" href="#references" title="Permanent link">¶</a></h3>
<ul>
<li><em>Gunasekar, S., et al. (2023). Textbooks Are All You Need (Phi-1).</em></li>
<li><em>Chen, W., et al. (2022). Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.</em></li>
<li><em>Liu, H., et al. (2023). Visual Instruction Tuning (LLaVA).</em></li>
<li><em>Shumailov, I., et al. (2023). The Curse of Recursion: Training on Generated Data Makes Models Forget.</em> (Important research on model collapse)</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../4_1_sft_data/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Chapter 9: Instruction Fine-tuning Data">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Chapter 9: Instruction Fine-tuning Data
              </div>
            </div>
          </a>
        
        
          
          <a href="../4_3_preference_data/" class="md-footer__link md-footer__link--next" aria-label="Next: Chapter 11: Human Preference Data">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Chapter 11: Human Preference Data
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": ["navigation.sections", "navigation.expand", "navigation.indexes", "navigation.top", "navigation.footer", "toc.follow", "search.suggest", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>