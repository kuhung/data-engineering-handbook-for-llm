{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üñºÔ∏è LLaVA Multimodal Instruction Tuning Data Construction Pipeline\n",
        "This Notebook integrates the complete workflow from **COCO Data Alignment** to **Automated Annotation using Multimodal Large Language Models (Qwen-VL)**. It is suitable for building training datasets for models like LLaVA and Qwen-VL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Environment Preparation\n",
        "Install necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install opencv-python tqdm openai numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Phase 1: Precision Coordinate Alignment (COCO Format to LLaVA)\n",
        "Convert standard COCO detection boxes `[x, y, w, h]` into LLaVA-formatted normalized coordinates `[ymin, xmin, ymax, xmax]` (0-1000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, glob, cv2\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "IMAGE_DIR = \"../data/images/\"\n",
        "ANNOTATION_FILE = \"../data/annotations/instances_val2017.json\"\n",
        "OUTPUT_ALIGN_FILE = \"../data/llava_instruct_aligned.json\"\n",
        "\n",
        "def convert_bbox(bbox, width, height):\n",
        "    x, y, w, h = bbox\n",
        "    xmin = int((x / width) * 1000)\n",
        "    ymin = int((y / height) * 1000)\n",
        "    xmax = int((x + w) / width * 1000)\n",
        "    ymax = int((y + h) / height * 1000)\n",
        "    return [max(0, min(1000, ymin)), max(0, min(1000, xmin)), \n",
        "            max(0, min(1000, ymax)), max(0, min(1000, xmax))]\n",
        "\n",
        "def load_coco():\n",
        "    with open(ANNOTATION_FILE, 'r') as f: coco = json.load(f)\n",
        "    img_to_anns = {}\n",
        "    cat_map = {cat['id']: cat['name'] for cat in coco['categories']}\n",
        "    for ann in coco['annotations']:\n",
        "        img_id = ann['image_id']\n",
        "        if img_id not in img_to_anns: img_to_anns[img_id] = []\n",
        "        img_to_anns[img_id].append({\"bbox\": ann['bbox'], \"label\": cat_map.get(ann['category_id'], \"object\")})\n",
        "    return img_to_anns\n",
        "\n",
        "if os.path.exists(ANNOTATION_FILE):\n",
        "    img_to_anns = load_coco()\n",
        "    dataset = []\n",
        "    image_paths = glob.glob(os.path.join(IMAGE_DIR, \"*.jpg\"))\n",
        "    \n",
        "    for img_path in tqdm(image_paths, desc=\"Aligning Data\"):\n",
        "        fname = os.path.basename(img_path)\n",
        "        try: image_id = int(fname.split('.')[0])\n",
        "        except: continue\n",
        "        anns = img_to_anns.get(image_id, [])\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None: continue\n",
        "        h, w, _ = img.shape\n",
        "        for ann in anns[:3]: # Extract 3 objects per image\n",
        "            box = convert_bbox(ann['bbox'], w, h)\n",
        "            dataset.append({\n",
        "                \"id\": f\"{image_id}_{ann['label']}\",\n",
        "                \"image\": fname,\n",
        "                \"conversations\": [\n",
        "                    {\"from\": \"human\", \"value\": f\"Where is the {ann['label']}? <image>\"},\n",
        "                    {\"from\": \"qwen\", \"value\": f\"The {ann['label']} is at {box}.\"}\n",
        "                ]\n",
        "            })\n",
        "    with open(OUTPUT_ALIGN_FILE, 'w') as f: json.dump(dataset, f, indent=2)\n",
        "    print(f\"Alignment complete, saved to {OUTPUT_ALIGN_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Phase 2: LLM Enhanced Annotation (Cognitive Data Generation)\n",
        "Leverage large models like Qwen2.5-VL to generate dialogues containing reasoning and detailed descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "from openai import OpenAI\n",
        "\n",
        "API_KEY = \"YOUR_API_KEY\"\n",
        "client = OpenAI(api_key=API_KEY, base_url=\"https://api.siliconflow.cn/v1\")\n",
        "\n",
        "def encode_img(p): \n",
        "    with open(p, \"rb\") as f: return base64.b64encode(f.read()).decode('utf-8')\n",
        "\n",
        "def generate_llava_json(img_path):\n",
        "    b64 = encode_img(img_path)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"Return JSON directly, including two rounds of dialogue: describing the image and detailed questioning. Use the [ymin, xmin, ymax, xmax] format.\"},\n",
        "                  {\"role\": \"user\", \"content\": [{\"type\":\"text\",\"text\":\"Analyze this image\"}, \n",
        "                                               {\"type\":\"image_url\",\"image_url\":{\"url\":f\"data:image/jpeg;base64,{b64}\"}}]}]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Phase 3: Interleaved Image Data\n",
        "Construct dialogues with interleaved input of two images to train the model's comparative and logical reasoning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "def generate_interleaved(img1, img2):\n",
        "    prompt = \"Compare these two images.\"\n",
        "    # Call the model and construct LLaVA Interleaved format...\n",
        "    # Example result below:\n",
        "    return {\n",
        "        \"id\": \"compare_001\",\n",
        "        \"image\": [os.path.basename(img1), os.path.basename(img2)],\n",
        "        \"conversations\": [\n",
        "            {\"from\": \"human\", \"value\": \"Image 1: <image>\\nImage 2: <image>\\nWhat is the difference?\"},\n",
        "            {\"from\": \"qwen\", \"value\": \"The first image is a city, the second is a park.\"}\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Phase 4: Debug Visualization\n",
        "Read the generated JSON file and project the bounding boxes back onto the images to verify the precision of the LLM-generated coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "VIZ_OUT = \"../data/viz_debug/\"\n",
        "os.makedirs(VIZ_OUT, exist_ok=True)\n",
        "\n",
        "with open(OUTPUT_ALIGN_FILE, 'r') as f: data = json.load(f)\n",
        "\n",
        "for entry in data[:5]: # Sample 5 images for verification\n",
        "    img = cv2.imread(os.path.join(IMAGE_DIR, entry['image']))\n",
        "    h, w, _ = img.shape\n",
        "    for turn in entry['conversations']:\n",
        "        bboxes = re.findall(r'\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]', turn['value'])\n",
        "        for b in bboxes:\n",
        "            ymin, xmin, ymax, xmax = [int(x) for x in b]\n",
        "            cv2.rectangle(img, (int(xmin*w/1000), int(ymin*h/1000)), \n",
        "                          (int(xmax*w/1000), int(ymax*h/1000)), (0, 255, 0), 2)\n",
        "    cv2.imwrite(os.path.join(VIZ_OUT, f\"viz_{entry['image']}\"), img)\n",
        "print(f\"Verification images saved to {VIZ_OUT}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}