{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ⚖️ Legal Domain LLM SFT Data Construction Pipeline\n",
        "This Notebook integrates the complete workflow from **PDF raw text extraction** to **diverse instruction generation (Instruct Tuning)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Environment Preparation\n",
        "Install necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pdfplumber tqdm openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Phase 1: PDF Data Cleaning and Parsing\n",
        "This section is responsible for reading PDF files, removing headers, footers, and page numbers, and using regex to extract legal articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Path Configuration ---\n",
        "RAW_DATA_DIR = '../data/raw' \n",
        "PROCESSED_DATA_DIR = '../data/processed'\n",
        "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
        "\n",
        "def clean_text_smart(text):\n",
        "    if not text: return \"\"\n",
        "    # A. Remove reference citation numbers\n",
        "    text = re.sub(r'\\[\\s*\\d+(?:[-–,]\\d+)*\\s*\\]', '', text)\n",
        "    text = re.sub(r'［\\s*\\d+(?:[-–,]\\d+)*\\s*］', '', text)\n",
        "    # B. Remove page numbers embedded in the middle of text\n",
        "    text = re.sub(r'(?:^|\\s|\\\\n)[-—–－]\\s*\\d+\\s*[-—–－](?=\\s|\\\\n|$)', ' ', text)\n",
        "    # C. Remove isolated line-level page numbers\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = [line for line in lines if not re.fullmatch(r'[-—–－\\s\\d]+', line.strip())]\n",
        "    text = '\\n'.join(cleaned_lines)\n",
        "    # D. Fix broken Chinese word segmentation\n",
        "    pattern_broken_zh = r'([\\u4e00-\\u9fa5])\\s+([\\u4e00-\\u9fa5])'\n",
        "    text = re.sub(pattern_broken_zh, r'\\1\\2', text)\n",
        "    text = re.sub(pattern_broken_zh, r'\\1\\2', text) \n",
        "    # E. Normalize whitespace characters\n",
        "    text = re.sub(r'[ \\t\\r\\f]+', ' ', text) \n",
        "    return text.strip()\n",
        "\n",
        "def process_legal_doc(file_path):\n",
        "    filename = os.path.basename(file_path)\n",
        "    full_text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in tqdm(pdf.pages, desc=f\"Parsing {filename}\", leave=False):\n",
        "                width, height = page.width, page.height\n",
        "                bbox = (0, height * 0.05, width, height * 0.95)\n",
        "                try:\n",
        "                    page_crop = page.crop(bbox=bbox)\n",
        "                    text = page_crop.extract_text()\n",
        "                    if text: full_text += \"\\n\" + text\n",
        "                except: continue\n",
        "    except Exception as e:\n",
        "        print(f\"Read failed: {e}\")\n",
        "        return []\n",
        "    \n",
        "    full_text = clean_text_smart(full_text)\n",
        "    pattern = r\"(第[0-9零一二三四五六七八九十百千]+条[\\s\\S]*?)(?=第[0-9零一二三四五六七八九十百千]+条|$)\"\n",
        "    matches = re.findall(pattern, full_text)\n",
        "    \n",
        "    return [{\"source\": filename, \"type\": \"legal_article\", \"content\": re.sub(r'\\s+', ' ', m).strip()} \n",
        "            for m in matches if len(m) > 15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute parsing loop\n",
        "if os.path.exists(RAW_DATA_DIR):\n",
        "    files = [f for f in os.listdir(RAW_DATA_DIR) if f.lower().endswith('.pdf')]\n",
        "    all_chunks = []\n",
        "    for filename in tqdm(files, desc=\"Total Parsing Progress\"):\n",
        "        if \"法\" in filename: # Only process files containing 'Law' in the name\n",
        "            chunks = process_legal_doc(os.path.join(RAW_DATA_DIR, filename))\n",
        "            all_chunks.extend(chunks)\n",
        "    \n",
        "    # Save intermediate results\n",
        "    output_path = os.path.join(PROCESSED_DATA_DIR, 'raw_chunks.jsonl')\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for chunk in all_chunks:\n",
        "            f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
        "    print(f\"✅ Cleaning complete, total of {len(all_chunks)} legal articles obtained.\")\n",
        "else:\n",
        "    print(\"❌ raw directory not found, please check the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Phase 2: Diverse Instruction Data Generation (SFT)\n",
        "Use LLM to transform legal articles into: **Case Analysis, Document Drafting, and Concept Explanation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Fill in your API Configuration ---\n",
        "API_KEY = \"Your_API_Key\"\n",
        "BASE_URL = \"https://api.siliconflow.cn/v1\"\n",
        "MODEL_NAME = \"deepseek-ai/DeepSeek-V3\"\n",
        "\n",
        "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
        "\n",
        "# Prompt Templates (Case/Doc/Concept)\n",
        "PROMPTS = {\n",
        "    \"case_analysis\": \"You are a senior lawyer. Please read this legal article: {content}. Construct a consultation case involving multiple conflicting parties. User: Describe the case. Assistant: <Thought Process> Analysis Logic + <Legal Advice> Conclusion.\",\n",
        "    \"doc_drafting\": \"You are a lawyer. Based on this legal article: {content}, construct User: Request to draft related document. Assistant: <Thought Process> Key Points + <Document Body> Content.\",\n",
        "    \"concept_explain\": \"You are a professor. Based on this legal article: {content}, construct User: Layperson asking about a concept. Assistant: <Thought Process> Deconstruction + <Simple Explanation> Examples.\"\n",
        "}\n",
        "\n",
        "def generate_sft_data(chunk):\n",
        "    task_type = random.choices([\"case_analysis\", \"doc_drafting\", \"concept_explain\"], weights=[0.6, 0.2, 0.2])[0]\n",
        "    prompt_tpl = PROMPTS[task_type]\n",
        "    \n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a legal expert data construction assistant. Please return JSON format, including 'instruction' and 'output' fields.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt_tpl.format(content=chunk['content'])}\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        data = json.loads(response.choices[0].message.content)\n",
        "        return {\n",
        "            \"instruction\": data.get(\"instruction\", \"\"),\n",
        "            \"output\": data.get(\"output\", \"\"),\n",
        "            \"task_type\": task_type,\n",
        "            \"source\": chunk.get('source')\n",
        "        }\n",
        "    except: return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute generation\n",
        "sft_items = []\n",
        "test_chunks = all_chunks[:10]  # Test first 10 items first\n",
        "\n",
        "for chunk in tqdm(test_chunks, desc=\"LLM Generating\"):\n",
        "    item = generate_sft_data(chunk)\n",
        "    if item: sft_items.append(item)\n",
        "    time.sleep(0.2)\n",
        "\n",
        "# Save final SFT data\n",
        "sft_path = os.path.join(PROCESSED_DATA_DIR, 'domain_expert_sft.jsonl')\n",
        "with open(sft_path, 'w', encoding='utf-8') as f:\n",
        "    for item in sft_items:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"✅ Generation complete! File saved to: {sft_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}