{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Data Scraping and Cleaning Pipeline (Data Processing Pipeline)\n",
        "\n",
        "Before starting, please ensure that all necessary dependencies are installed:\n",
        "`pip install requests tqdm warcio trafilatura datasketch ray fasttext kenlm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ================= 1. Global Path Configuration =================\n",
        "CURRENT_DIR = os.getcwd() \n",
        "PROJECT_ROOT = CURRENT_DIR \n",
        "\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
        "RAW_DIR = os.path.join(DATA_DIR, \"raw\")\n",
        "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
        "\n",
        "# Ensure base directories exist\n",
        "for d in [RAW_DIR, PROCESSED_DIR, MODEL_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Working directory set to: {CURRENT_DIR}\")\n",
        "print(f\"üìÅ Raw data directory: {RAW_DIR}\")\n",
        "print(f\"üìÅ Processed data directory: {PROCESSED_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 2. Download Common Crawl Data =================\n",
        "CRAWL_ID = \"CC-MAIN-2023-50\" \n",
        "NUM_FILES_TO_DOWNLOAD = 1\n",
        "BASE_URL = \"https://data.commoncrawl.org\"\n",
        "\n",
        "def get_warc_file_paths(crawl_id, num_files):\n",
        "    paths_url = f\"{BASE_URL}/crawl-data/{crawl_id}/warc.paths.gz\"\n",
        "    print(f\"üì° Fetching file index: {paths_url} ...\")\n",
        "    try:\n",
        "        response = requests.get(paths_url, stream=True, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        paths = []\n",
        "        with gzip.open(response.raw, 'rt', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= num_files: break\n",
        "                paths.append(line.strip())\n",
        "        return paths\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to fetch index: {e}\")\n",
        "        return []\n",
        "\n",
        "def download_file(url, output_dir):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    local_path = os.path.join(output_dir, local_filename)\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"‚ö†Ô∏è File already exists, skipping: {local_filename}\")\n",
        "        return local_path\n",
        "\n",
        "    print(f\"‚¨áÔ∏è Starting download: {local_filename}\")\n",
        "    try:\n",
        "        with requests.get(url, stream=True, timeout=30) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "            with open(local_path, 'wb') as f, tqdm(\n",
        "                desc=local_filename, total=total_size, unit='iB',\n",
        "                unit_scale=True, unit_divisor=1024\n",
        "            ) as bar:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    size = f.write(chunk)\n",
        "                    bar.update(size)\n",
        "        print(f\"‚úÖ Download complete: {local_path}\")\n",
        "        return local_path\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Download failed {url}: {e}\")\n",
        "        if os.path.exists(local_path): os.remove(local_path)\n",
        "        return None\n",
        "\n",
        "warc_paths = get_warc_file_paths(CRAWL_ID, NUM_FILES_TO_DOWNLOAD)\n",
        "if warc_paths:\n",
        "    print(f\"üéØ Planning to download {len(warc_paths)} files to {RAW_DIR} ...\")\n",
        "    for relative_path in warc_paths:\n",
        "        full_url = f\"{BASE_URL}/{relative_path}\"\n",
        "        download_file(full_url, RAW_DIR)\n",
        "    print(\"\\nüéâ Data preparation stage complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 3. Extract WARC Content =================\n",
        "from warcio.archiveiterator import ArchiveIterator\n",
        "import trafilatura\n",
        "\n",
        "LIMIT_RECORDS = 10000 \n",
        "OUTPUT_FILE_STEP2 = os.path.join(PROCESSED_DIR, \"extracted_data.jsonl\")\n",
        "\n",
        "def extract_text_from_warc(warc_path, output_path, limit=None):\n",
        "    print(f\"üöÄ Starting processing: {warc_path}\")\n",
        "    counter, success_count = 0, 0\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as out_f:\n",
        "        with open(warc_path, 'rb') as stream:\n",
        "            for record in tqdm(ArchiveIterator(stream), desc=\"Processing Records\"):\n",
        "                if record.rec_type == 'response':\n",
        "                    content_type = record.http_headers.get_header('Content-Type')\n",
        "                    if not content_type or 'text/html' not in content_type:\n",
        "                        continue\n",
        "                    try:\n",
        "                        content = record.content_stream().read()\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                        \n",
        "                    text = trafilatura.extract(\n",
        "                        content, include_comments=False, \n",
        "                        include_tables=False, no_fallback=False\n",
        "                    )\n",
        "                    \n",
        "                    if text and len(text.strip()) > 0:\n",
        "                        url = record.rec_headers.get_header('WARC-Target-URI')\n",
        "                        data = {\"url\": url, \"text\": text}\n",
        "                        out_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "                        success_count += 1\n",
        "                \n",
        "                counter += 1\n",
        "                if limit and counter >= limit: break\n",
        "    \n",
        "    print(f\"\\n‚úÖ Processing complete! Records scanned: {counter}, Successfully extracted: {success_count}\")\n",
        "\n",
        "files = [f for f in os.listdir(RAW_DIR) if f.endswith('.warc.gz')]\n",
        "if files:\n",
        "    input_warc_path = os.path.join(RAW_DIR, files[0])\n",
        "    extract_text_from_warc(input_warc_path, OUTPUT_FILE_STEP2, LIMIT_RECORDS)\n",
        "else:\n",
        "    print(\"‚ùå No warc.gz files found in the raw directory. Please check if the previous step was successful.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 4. Basic Rule-Based Cleaning =================\n",
        "INPUT_FILE_STEP3 = OUTPUT_FILE_STEP2\n",
        "OUTPUT_FILE_STEP3 = os.path.join(PROCESCESSED_DIR, \"clean_data.jsonl\")\n",
        "\n",
        "def is_high_quality(text):\n",
        "    if len(text) < 100 or len(text) > 2_000_000: return False\n",
        "        \n",
        "    words = text.split()\n",
        "    if len(words) == 0: return False\n",
        "    mean_word_len = sum(len(w) for w in words) / len(words)\n",
        "    if mean_word_len > 15: return False\n",
        "\n",
        "    code_symbols = {'{', '}', '[', ']', '<', '>', '\\\\'}\n",
        "    symbol_count = sum(1 for char in text if char in code_symbols)\n",
        "    if symbol_count / len(text) > 0.1: return False\n",
        "\n",
        "    bad_phrases = [\"lorem ipsum\", \"javascript is disabled\", \"enable cookies\",\n",
        "                   \"403 forbidden\", \"404 not found\", \"access denied\", \"rights reserved\"]\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    for phrase in bad_phrases:\n",
        "        if phrase in text_lower and len(text) < 500:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "if os.path.exists(INPUT_FILE_STEP3):\n",
        "    stats = {\"total\": 0, \"kept\": 0, \"dropped\": 0}\n",
        "    with open(INPUT_FILE_STEP3, 'r', encoding='utf-8') as f_in, \\\n",
        "         open(OUTPUT_FILE_STEP3, 'w', encoding='utf-8') as f_out:\n",
        "        \n",
        "        for line in tqdm(f_in, desc=\"Cleaning Data\"):\n",
        "            stats[\"total\"] += 1\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                if is_high_quality(item.get(\"text\", \"\")):\n",
        "                    f_out.write(line)\n",
        "                    stats[\"kept\"] += 1\n",
        "                else:\n",
        "                    stats[\"dropped\"] += 1\n",
        "            except json.JSONDecodeError: continue\n",
        "\n",
        "    print(f\"\\n‚úÖ Cleaning complete! Total: {stats['total']}, Dropped: {stats['dropped']}, Kept: {stats['kept']}\")\n",
        "else:\n",
        "    print(f\"‚ùå Input file not found: {INPUT_FILE_STEP3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 5. Global Ray LSH Deduplication =================\n",
        "import ray\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "INPUT_FILE_STEP4 = OUTPUT_FILE_STEP3\n",
        "OUTPUT_FILE_STEP4 = os.path.join(PROCESSED_DIR, \"deduplicated_data.jsonl\")\n",
        "\n",
        "NUM_PERM = 128 \n",
        "THRESHOLD = 0.8  \n",
        "\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "@ray.remote\n",
        "def process_batch(lines, batch_id):\n",
        "    results = []\n",
        "    for line in lines:\n",
        "        try:\n",
        "            item = json.loads(line)\n",
        "            m = MinHash(num_perm=NUM_PERM)\n",
        "            for w in item['text'].split():\n",
        "                m.update(w.encode('utf8'))\n",
        "            results.append((item['url'], m, item['text']))\n",
        "        except Exception:\n",
        "            continue\n",
        "    return results\n",
        "\n",
        "if os.path.exists(INPUT_FILE_STEP4):\n",
        "    print(\"üöÄ Phase 1: Parallel computing MinHash signatures...\")\n",
        "    with open(INPUT_FILE_STEP4, 'r', encoding='utf-8') as f:\n",
        "        all_lines = f.readlines()\n",
        "    \n",
        "    batch_size = 1000\n",
        "    batches = [all_lines[i:i + batch_size] for i in range(0, len(all_lines), batch_size)]\n",
        "    futures = [process_batch.remote(batch, i) for i, batch in enumerate(batches)]\n",
        "    \n",
        "    processed_batches = ray.get(futures)\n",
        "    results = [item for batch in processed_batches for item in batch]\n",
        "\n",
        "    print(\"\\nüöÄ Phase 2: Building LSH index and deduplicating...\")\n",
        "    lsh = MinHashLSH(threshold=THRESHOLD, num_perm=NUM_PERM)\n",
        "    unique_records, duplicate_count = [], 0\n",
        "    \n",
        "    for url, minhash, text in tqdm(results, desc=\"LSH Deduplication\"):\n",
        "        if len(lsh.query(minhash)) > 0:\n",
        "            duplicate_count += 1\n",
        "        else:\n",
        "            lsh.insert(url, minhash)\n",
        "            unique_records.append({\"url\": url, \"text\": text})\n",
        "\n",
        "    print(f\"\\n‚úÖ Deduplication complete! Duplicates found: {duplicate_count}, Valid remaining: {len(unique_records)}\")\n",
        "    \n",
        "    with open(OUTPUT_FILE_STEP4, 'w', encoding='utf-8') as f:\n",
        "        for item in unique_records:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    ray.shutdown()\n",
        "else:\n",
        "    print(f\"‚ùå File not found: {INPUT_FILE_STEP4}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 6. Language Identification with FastText =================\n",
        "import fasttext\n",
        "\n",
        "INPUT_FILE_STEP5 = OUTPUT_FILE_STEP4\n",
        "FASTTEXT_MODEL_PATH = os.path.join(MODEL_DIR, 'lid.176.ftz')\n",
        "\n",
        "if os.path.exists(FASTTEXT_MODEL_PATH) and os.path.exists(INPUT_FILE_STEP5):\n",
        "    print(f\"Loading language model: {FASTTEXT_MODEL_PATH}\")\n",
        "    fasttext.FastText.eprint = lambda x: None\n",
        "    model = fasttext.load_model(FASTTEXT_MODEL_PATH)\n",
        "    \n",
        "    files_out = {\n",
        "        'en': open(os.path.join(PROCESSED_DIR, 'data_en.jsonl'), 'w', encoding='utf-8'),\n",
        "        'zh': open(os.path.join(PROCESSED_DIR, 'data_zh.jsonl'), 'w', encoding='utf-8'),\n",
        "        'others': open(os.path.join(PROCESSED_DIR, 'data_others.jsonl'), 'w', encoding='utf-8')\n",
        "    }\n",
        "\n",
        "    count = 0\n",
        "    with open(INPUT_FILE_STEP5, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Splitting Languages\"):\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                text = data.get('text', '').replace('\\n', ' ')\n",
        "                if not text: continue\n",
        "\n",
        "                predictions = model.predict(text, k=1) \n",
        "                lang = predictions[0][0].replace('__label__', '')\n",
        "                \n",
        "                if lang == 'en': files_out['en'].write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "                elif lang == 'zh': files_out['zh'].write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "                else:\n",
        "                    data['detected_lang'] = lang \n",
        "                    files_out['others'].write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
        "                count += 1\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    for f in files_out.values(): f.close()\n",
        "    print(\"‚úÖ Processing complete! Generated data_en.jsonl, data_zh.jsonl, and data_others.jsonl\")\n",
        "else:\n",
        "    print(f\"‚ùå FastText model file or input file not found. Please verify.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 7. KenLM-Based Quality Filtering =================\n",
        "import kenlm\n",
        "\n",
        "INPUT_FILE_STEP6 = os.path.join(PROCESSED_DIR, \"data_en.jsonl\")\n",
        "OUTPUT_FILE_STEP6 = os.path.join(PROCESSED_DIR, \"final_data.jsonl\")\n",
        "KENLM_MODEL_PATH = os.path.join(MODEL_DIR, \"en.arpa.bin\")\n",
        "PERPLEXITY_THRESHOLD = -6.0\n",
        "\n",
        "if os.path.exists(INPUT_FILE_STEP6) and os.path.exists(KENLM_MODEL_PATH):\n",
        "    print(f\"üöÄ Loading KenLM model: {KENLM_MODEL_PATH} ...\")\n",
        "    lm_model = kenlm.Model(KENLM_MODEL_PATH)\n",
        "    \n",
        "    stats = {\"total\": 0, \"kept\": 0, \"dropped\": 0}\n",
        "    print(f\"üîÑ Starting quality filtering (Threshold: {PERPLEXITY_THRESHOLD})...\")\n",
        "    \n",
        "    with open(INPUT_FILE_STEP6, 'r', encoding='utf-8') as f_in, \\\n",
        "         open(OUTPUT_FILE_STEP6, 'w', encoding='utf-8') as f_out:\n",
        "        \n",
        "        for line in tqdm(f_in, desc=\"KenLM Filtering\"):\n",
        "            stats[\"total\"] += 1\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                text = item.get(\"text\", \"\")\n",
        "                words = text.split()\n",
        "                num_words = len(words)\n",
        "                \n",
        "                if num_words < 3:\n",
        "                    stats[\"dropped\"] += 1\n",
        "                    continue\n",
        "\n",
        "                normalized_score = lm_model.score(text) / num_words\n",
        "                \n",
        "                if normalized_score > PERPLEXITY_THRESHOLD:\n",
        "                    item[\"perplexity_score\"] = normalized_score\n",
        "                    f_out.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "                    stats[\"kept\"] += 1\n",
        "                else:\n",
        "                    stats[\"dropped\"] += 1\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    print(\"\\nüéâ Pipeline execution finished!\")\n",
        "    print(f\"   Total Input: {stats['total']}, Kept (High Quality): {stats['kept']}, Dropped: {stats['dropped']}\")\n",
        "else:\n",
        "    print(f\"‚ùå Input data or KenLM model not found: {KENLM_MODEL_PATH}.\")"
      ]
    }
  ]
}