{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚖️ 法律领域大模型 SFT 数据构造流水线\n",
    "本 Notebook 整合了从 **PDF 原文提取** 到 **多样化指令生成 (Instruct Tuning)** 的完整流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 环境准备\n",
    "安装必要的依赖包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfplumber tqdm openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 第一阶段：PDF 数据清洗与解析\n",
    "该部分负责读取 PDF 文件，移除页眉页脚、页码，并利用正则提取法律条文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 配置路径 ---\n",
    "RAW_DATA_DIR = '../data/raw' \n",
    "PROCESSED_DATA_DIR = '../data/processed'\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text_smart(text):\n",
    "    if not text: return \"\"\n",
    "    # A. 去除参考文献引用标号\n",
    "    text = re.sub(r'\\[\\s*\\d+(?:[-–,]\\d+)*\\s*\\]', '', text)\n",
    "    text = re.sub(r'［\\s*\\d+(?:[-–,]\\d+)*\\s*］', '', text)\n",
    "    # B. 去除嵌在文本中间的页码\n",
    "    text = re.sub(r'(?:^|\\s|\\\\n)[-—–－]\\s*\\d+\\s*[-—–－](?=\\s|\\\\n|$)', ' ', text)\n",
    "    # C. 去除孤立的行级页码\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = [line for line in lines if not re.fullmatch(r'[-—–－\\s\\d]+', line.strip())]\n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "    # D. 修复中文断词\n",
    "    pattern_broken_zh = r'([\\u4e00-\\u9fa5])\\s+([\\u4e00-\\u9fa5])'\n",
    "    text = re.sub(pattern_broken_zh, r'\\1\\2', text)\n",
    "    text = re.sub(pattern_broken_zh, r'\\1\\2', text) \n",
    "    # E. 规范化空白字符\n",
    "    text = re.sub(r'[ \\t\\r\\f]+', ' ', text) \n",
    "    return text.strip()\n",
    "\n",
    "def process_legal_doc(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in tqdm(pdf.pages, desc=f\"解析 {filename}\", leave=False):\n",
    "                width, height = page.width, page.height\n",
    "                bbox = (0, height * 0.05, width, height * 0.95)\n",
    "                try:\n",
    "                    page_crop = page.crop(bbox=bbox)\n",
    "                    text = page_crop.extract_text()\n",
    "                    if text: full_text += \"\\n\" + text\n",
    "                except: continue\n",
    "    except Exception as e:\n",
    "        print(f\"读取失败: {e}\")\n",
    "        return []\n",
    "    \n",
    "    full_text = clean_text_smart(full_text)\n",
    "    pattern = r\"(第[0-9零一二三四五六七八九十百千]+条[\\s\\S]*?)(?=第[0-9零一二三四五六七八九十百千]+条|$)\"\n",
    "    matches = re.findall(pattern, full_text)\n",
    "    \n",
    "    return [{\"source\": filename, \"type\": \"legal_article\", \"content\": re.sub(r'\\s+', ' ', m).strip()} \n",
    "            for m in matches if len(m) > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行解析循环\n",
    "if os.path.exists(RAW_DATA_DIR):\n",
    "    files = [f for f in os.listdir(RAW_DATA_DIR) if f.lower().endswith('.pdf')]\n",
    "    all_chunks = []\n",
    "    for filename in tqdm(files, desc=\"总解析进度\"):\n",
    "        if \"法\" in filename:\n",
    "            chunks = process_legal_doc(os.path.join(RAW_DATA_DIR, filename))\n",
    "            all_chunks.extend(chunks)\n",
    "    \n",
    "    # 保存中间结果\n",
    "    output_path = os.path.join(PROCESSED_DATA_DIR, 'raw_chunks.jsonl')\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in all_chunks:\n",
    "            f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "    print(f\"✅ 清洗完成，共得到 {len(all_chunks)} 条法条数据。\")\n",
    "else:\n",
    "    print(\"❌ 未找到 raw 目录，请检查路径。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 第二阶段：多样化指令数据生成 (SFT)\n",
    "使用 LLM 将法条转化为：**案例分析、文书起草、概念解释**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- 填写你的 API 配置 ---\n",
    "API_KEY = \"你的API密钥\"\n",
    "BASE_URL = \"https://api.siliconflow.cn/v1\"\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# 提示词模板 (Case/Doc/Concept)\n",
    "PROMPTS = {\n",
    "    \"case_analysis\": \"你是一位资深律师。请阅读法条：{content}。请构造一个包含多方冲突的咨询案例。User: 描述案情。Assistant: <思考过程>分析逻辑 + <法律建议>结论。\",\n",
    "    \"doc_drafting\": \"你是一位律师。请根据法条：{content}。构造 User: 要求起草相关文书。Assistant: <思考过程>要点 + <文书正文>内容。\",\n",
    "    \"concept_explain\": \"你是一位教授。请根据法条：{content}。构造 User: 小白提问概念。Assistant: <思考过程>拆解 + <通俗解释>举例。\"\n",
    "}\n",
    "\n",
    "def generate_sft_data(chunk):\n",
    "    task_type = random.choices([\"case_analysis\", \"doc_drafting\", \"concept_explain\"], weights=[0.6, 0.2, 0.2])[0]\n",
    "    prompt_tpl = PROMPTS[task_type]\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"你是一个法律专家数据构造助手。请返回 JSON 格式，包含 instruction 和 output 两个字段。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_tpl.format(content=chunk['content'])}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        data = json.loads(response.choices[0].message.content)\n",
    "        return {\n",
    "            \"instruction\": data.get(\"instruction\", \"\"),\n",
    "            \"output\": data.get(\"output\", \"\"),\n",
    "            \"task_type\": task_type,\n",
    "            \"source\": chunk.get('source')\n",
    "        }\n",
    "    except: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行生成\n",
    "sft_items = []\n",
    "test_chunks = all_chunks[:10]  # 先测试前10条\n",
    "\n",
    "for chunk in tqdm(test_chunks, desc=\"LLM 生成中\"):\n",
    "    item = generate_sft_data(chunk)\n",
    "    if item: sft_items.append(item)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "# 保存最终 SFT 数据\n",
    "sft_path = os.path.join(PROCESSED_DATA_DIR, 'domain_expert_sft.jsonl')\n",
    "with open(sft_path, 'w', encoding='utf-8') as f:\n",
    "    for item in sft_items:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"✅ 生成完毕！文件已保存至: {sft_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
