import pdfplumber
import os
import re
import json
from tqdm import tqdm

# --- 1. é…ç½®è·¯å¾„ ---
RAW_DATA_DIR = '../data/raw' 
PROCESSED_DATA_DIR = '../data/processed'

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

# --- 2. æ ¸å¿ƒç»„ä»¶ï¼šæ™ºèƒ½æ¸…æ´—å‡½æ•° (å¢å¼ºä¿®å¤ç‰ˆ) ---
def clean_text_smart(text):
    """
    é’ˆå¯¹ PDF æå–çš„æ–‡æœ¬è¿›è¡Œæ·±åº¦æ¸…æ´—
    è§£å†³ï¼šé¡µç æ··å…¥ã€å¼•ç”¨æ ‡å·æ®‹ç•™ã€ä¸­æ–‡è¢«ç©ºæ ¼åˆ‡æ–­ç­‰é—®é¢˜
    """
    if not text:
        return ""

    # A. å»é™¤å‚è€ƒæ–‡çŒ®å¼•ç”¨æ ‡å·
    # åŒ¹é… [1]ã€[1,2]ã€[1-3]ã€ï¼»12ï¼½ è¿™ç§æ ¼å¼
    text = re.sub(r'\[\s*\d+(?:[-â€“,]\d+)*\s*\]', '', text)
    text = re.sub(r'ï¼»\s*\d+(?:[-â€“,]\d+)*\s*ï¼½', '', text)

    # B. ã€æ–°å¢ã€‘å»é™¤åµŒåœ¨æ–‡æœ¬ä¸­é—´çš„é¡µç  (é’ˆå¯¹ - 195 - é—®é¢˜)
    # è§£é‡Šï¼š
    # (?:^|\s|\\n)  : å‰é¢å¿…é¡»æ˜¯å¼€å¤´ã€ç©ºç™½æˆ–æ¢è¡Œ
    # [-â€”â€“ï¼]       : åŒ¹é…å„ç§ç ´æŠ˜å·ï¼ˆåŒ…æ‹¬å…¨è§’ ï¼ï¼ŒåŠè§’ -ï¼ŒEm dash â€” ç­‰ï¼‰
    # \s*\d+\s* : ä¸­é—´æ˜¯æ•°å­—ï¼Œå…è®¸æœ‰ç©ºæ ¼
    # [-â€”â€“ï¼]       : åé¢å¿…é¡»è·Ÿç€é—­åˆçš„ç ´æŠ˜å·
    # (?=\s|\\n|$)  : åé¢å¿…é¡»æ˜¯ç©ºç™½ã€æ¢è¡Œæˆ–ç»“å°¾ (é¿å…è¯¯åˆ  "Item-1-A" è¿™ç§ç¼–å·)
    text = re.sub(r'(?:^|\s|\\n)[-â€”â€“ï¼]\s*\d+\s*[-â€”â€“ï¼](?=\s|\\n|$)', ' ', text)

    # C. å»é™¤å­¤ç«‹çš„è¡Œçº§é¡µç 
    # ç­–ç•¥ï¼šå¦‚æœä¸€è¡Œåªæœ‰æ•°å­—å’Œæ¨ªæ ï¼Œåˆ æ‰ã€‚
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        s_line = line.strip()
        # å¦‚æœè¿™ä¸€è¡Œå…¨æ˜¯æ•°å­—å’Œå„ç§ç¬¦å·ï¼ˆç ´æŠ˜å·ã€ç©ºæ ¼ï¼‰ï¼Œåˆ¤å®šä¸ºé¡µç è¡Œ
        if re.fullmatch(r'[-â€”â€“ï¼\s\d]+', s_line):
            continue
        cleaned_lines.append(line)
    text = '\n'.join(cleaned_lines)

    # D. ä¿®å¤ä¸­æ–‡æ–­è¯ (æ ¸å¿ƒä¿®å¤)
    # é€»è¾‘ï¼šæŸ¥æ‰¾ [ä¸­æ–‡] [ç©ºæ ¼] [ä¸­æ–‡] çš„æ¨¡å¼ï¼ŒæŠŠç©ºæ ¼å»æ‰
    # æ‰§è¡Œä¸¤æ¬¡ä»¥å¤„ç†è¿ç»­çš„æ–­è¯ (å¦‚ "A B C" -> "AB C" -> "ABC")
    pattern_broken_zh = r'([\u4e00-\u9fa5])\s+([\u4e00-\u9fa5])'
    text = re.sub(pattern_broken_zh, r'\1\2', text)
    text = re.sub(pattern_broken_zh, r'\1\2', text) 

    # E. è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    # è¿™é‡Œçš„ç­–ç•¥æ˜¯ä¿ç•™æ¢è¡Œç¬¦ \n ä»¥ç»´æŒæ®µè½ç»“æ„ï¼Œä½†æŠŠè¡Œå†…çš„å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    # ä¹Ÿå°±æ˜¯ï¼šæŠŠ "word1   word2" å˜æˆ "word1 word2"ï¼Œä½†ä¸æŠŠ "\n" å˜æˆç©ºæ ¼
    # å¦‚æœæƒ³å®Œå…¨å‹æ‰æˆä¸€è¡Œï¼Œå¯ä»¥ç”¨ re.sub(r'\s+', ' ', text)
    text = re.sub(r'[ \t\r\f]+', ' ', text) # ä»…åˆå¹¶æ°´å¹³ç©ºç™½ï¼Œä¿ç•™ \n
    
    return text.strip()

# --- 3. ç­–ç•¥ Aï¼šæ³•å¾‹æ–‡æ¡£å¤„ç† ---
def process_legal_doc(file_path):
    filename = os.path.basename(file_path)
    print(f"âš–ï¸ [æ³•å¾‹] æ­£åœ¨è§£æ: {filename}")
    full_text = ""
    
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in tqdm(pdf.pages, desc="è¯»å–é¡µé¢", leave=False):
                # è£å‰ªé¡µçœ‰é¡µè„š (ä¸Šä¸‹å„åˆ‡é™¤ 5%)
                width, height = page.width, page.height
                bbox = (0, height * 0.05, width, height * 0.95)
                try:
                    page_crop = page.crop(bbox=bbox)
                    text = page_crop.extract_text()
                    if text:
                        full_text += "\n" + text
                except Exception:
                    continue
    except Exception as e:
        print(f"âŒ æ— æ³•æ‰“å¼€æ–‡ä»¶ {filename}: {e}")
        return []

    # å…¨æ–‡æ¸…æ´—
    full_text = clean_text_smart(full_text)

    # æ­£åˆ™åŒ¹é…ï¼šç¬¬[æ•°å­—/ä¸­æ–‡]æ¡
    # ä¼˜åŒ–æ­£åˆ™ï¼šæ”¯æŒ "ç¬¬ä¸€æ¡" å’Œ "ç¬¬1æ¡"
    pattern = r"(ç¬¬[0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡[\s\S]*?)(?=ç¬¬[0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡|$)"
    matches = re.findall(pattern, full_text)
    
    chunks = []
    for match in matches:
        # å†æ¬¡æ¸…æ´—å•ä¸ªç‰‡æ®µå¤šä½™çš„ç©ºç™½
        cleaned = re.sub(r'\s+', ' ', match).strip()
        if len(cleaned) > 15: # ç¨å¾®æé«˜é˜ˆå€¼ï¼Œè¿‡æ»¤æ‚éŸ³
            chunks.append({
                "source": filename,
                "type": "legal_article",
                "content": cleaned
            })
            
    print(f"   => æå–åˆ° {len(chunks)} ä¸ªæ³•æ¡ç‰‡æ®µ")
    return chunks

# --- 4. ç­–ç•¥ Bï¼šåŒ»ç–—æŒ‡å—å¤„ç† (æ»‘åŠ¨çª—å£) ---
def process_medical_doc(file_path, chunk_size=500, overlap=100):
    filename = os.path.basename(file_path)
    print(f"âš•ï¸ [åŒ»ç–—] æ­£åœ¨è§£æ: {filename}")
    full_text = ""
    
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in tqdm(pdf.pages, desc="è¯»å–é¡µé¢", leave=False):
                width, height = page.width, page.height
                bbox = (0, height * 0.05, width, height * 0.95)
                try:
                    page_crop = page.crop(bbox=bbox)
                    text = page_crop.extract_text()
                    if text:
                        # æ‹¼æ¥æ—¶åŠ ä¸Šæ¢è¡Œï¼Œé˜²æ­¢è·¨é¡µç²˜è¿
                        full_text += text + "\n"
                except Exception:
                    continue
    except Exception as e:
        print(f"âŒ æ— æ³•æ‰“å¼€æ–‡ä»¶ {filename}: {e}")
        return []

    # å…¨æ–‡æ¸…æ´— (åœ¨åˆ‡ç‰‡å‰åšï¼Œä¿®å¤æ–­è¯æ•ˆæœæœ€å¥½)
    full_text = clean_text_smart(full_text)
    # å°†å¤šä½™æ¢è¡Œå‹æ‰ï¼Œæ–¹ä¾¿æ»‘åŠ¨çª—å£è®¡ç®—å­—æ•°
    full_text = re.sub(r'\s+', ' ', full_text)

    # --- æ»‘åŠ¨çª—å£åˆ‡åˆ† ---
    chunks = []
    total_len = len(full_text)
    start = 0
    
    pbar = tqdm(total=total_len, desc="åˆ‡åˆ†æ–‡æœ¬", unit="char", leave=False)
    
    while start < total_len:
        end = min(start + chunk_size, total_len)
        chunk_text = full_text[start:end]
        
        # æ™ºèƒ½æˆªæ–­ï¼šä¼˜å…ˆåœ¨å¥å·å¤„æˆªæ–­
        last_period = chunk_text.rfind('ã€‚')
        
        # åªæœ‰æˆªæ–­åé•¿åº¦ä¾ç„¶åˆç†ï¼ˆ> overlapï¼‰æ‰æˆªæ–­
        if last_period != -1 and (start + last_period) < total_len:
            current_length = last_period + 1
            if current_length > overlap:
                end = start + current_length
                chunk_text = full_text[start:end]
            
        if len(chunk_text) > 50: 
            chunks.append({
                "source": filename,
                "type": "medical_guide",
                "content": chunk_text
            })
        
        # æ­¥é•¿è®¡ç®— (é˜²æ­¢æ­»å¾ªç¯)
        step = len(chunk_text) - overlap
        if step <= 0:
            step = 1 # å¼ºåˆ¶å‰è¿›
            
        start += step
        pbar.update(step)

    pbar.close()
    print(f"   => åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
    return chunks

# --- 5. ä¸»ç¨‹åº ---
def main():
    if not os.path.exists(RAW_DATA_DIR):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºæ•°æ®ç›®å½• {RAW_DATA_DIR}")
        return

    files = [f for f in os.listdir(RAW_DATA_DIR) if f.lower().endswith('.pdf')]
    if not files:
        print("ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶ã€‚")
        return
        
    all_chunks = []
    print(f"ğŸš€ å¼€å§‹æ¸…æ´—å¤„ç†ï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶...")
    
    for filename in files:
        file_path = os.path.join(RAW_DATA_DIR, filename)
        
        # ç®€å•åˆ†ç±»é€»è¾‘
        if "æ³•" in filename and "æŒ‡å—" not in filename and "è§„èŒƒ" not in filename:
            chunks = process_legal_doc(file_path)
        else:
            chunks = process_medical_doc(file_path)
            
        all_chunks.extend(chunks)

    # ä¿å­˜
    output_file = os.path.join(PROCESSED_DATA_DIR, 'raw_chunks.jsonl')
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜è‡³ {output_file} ...")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for chunk in all_chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
            
    print(f"âœ… å¤„ç†å®Œæˆï¼æ€»è®¡ç”Ÿæˆ {len(all_chunks)} æ¡æ¸…æ´—åçš„æ•°æ®ã€‚")

if __name__ == "__main__":
    main()