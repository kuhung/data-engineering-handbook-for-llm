{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ç½‘é¡µæ•°æ®æŠ“å–ä¸æ¸…æ´—æµæ°´çº¿ (Data Processing Pipeline)\n\nåœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…äº†æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åŒ…ï¼š\n`pip install requests tqdm warcio trafilatura datasketch ray fasttext kenlm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport json\nimport gzip\nimport re\nimport time\nimport requests\nfrom tqdm.notebook import tqdm\n\n# ================= 1. å…¨å±€è·¯å¾„é…ç½® =================\nCURRENT_DIR = os.getcwd() \nPROJECT_ROOT = CURRENT_DIR \n\nDATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\nRAW_DIR = os.path.join(DATA_DIR, \"raw\")\nPROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\nMODEL_DIR = os.path.join(PROJECT_ROOT, \"models\")\n\n# ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨\nfor d in [RAW_DIR, PROCESSED_DIR, MODEL_DIR]:\n    os.makedirs(d, exist_ok=True)\n\nprint(f\"ğŸ“ å·¥ä½œç›®å½•å·²è®¾ç½®ä¸º: {CURRENT_DIR}\")\nprint(f\"ğŸ“ åŸå§‹æ•°æ®ç›®å½•: {RAW_DIR}\")\nprint(f\"ğŸ“ å¤„ç†åæ•°æ®ç›®å½•: {PROCESSED_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 2. ä¸‹è½½ Common Crawl æ•°æ® =================\nCRAWL_ID = \"CC-MAIN-2023-50\" \nNUM_FILES_TO_DOWNLOAD = 1\nBASE_URL = \"https://data.commoncrawl.org\"\n\ndef get_warc_file_paths(crawl_id, num_files):\n    paths_url = f\"{BASE_URL}/crawl-data/{crawl_id}/warc.paths.gz\"\n    print(f\"ğŸ“¡ æ­£åœ¨è·å–æ–‡ä»¶ç´¢å¼•: {paths_url} ...\")\n    try:\n        response = requests.get(paths_url, stream=True, timeout=10)\n        response.raise_for_status()\n        paths = []\n        with gzip.open(response.raw, 'rt', encoding='utf-8') as f:\n            for i, line in enumerate(f):\n                if i >= num_files: break\n                paths.append(line.strip())\n        return paths\n    except Exception as e:\n        print(f\"âŒ è·å–ç´¢å¼•å¤±è´¥: {e}\")\n        return []\n\ndef download_file(url, output_dir):\n    local_filename = url.split('/')[-1]\n    local_path = os.path.join(output_dir, local_filename)\n    if os.path.exists(local_path):\n        print(f\"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {local_filename}\")\n        return local_path\n\n    print(f\"â¬‡ï¸ å¼€å§‹ä¸‹è½½: {local_filename}\")\n    try:\n        with requests.get(url, stream=True, timeout=30) as r:\n            r.raise_for_status()\n            total_size = int(r.headers.get('content-length', 0))\n            with open(local_path, 'wb') as f, tqdm(\n                desc=local_filename, total=total_size, unit='iB',\n                unit_scale=True, unit_divisor=1024\n            ) as bar:\n                for chunk in r.iter_content(chunk_size=8192):\n                    size = f.write(chunk)\n                    bar.update(size)\n        print(f\"âœ… ä¸‹è½½å®Œæˆ: {local_path}\")\n        return local_path\n    except Exception as e:\n        print(f\"âŒ ä¸‹è½½å¤±è´¥ {url}: {e}\")\n        if os.path.exists(local_path): os.remove(local_path)\n        return None\n\nwarc_paths = get_warc_file_paths(CRAWL_ID, NUM_FILES_TO_DOWNLOAD)\nif warc_paths:\n    print(f\"ğŸ¯ è®¡åˆ’ä¸‹è½½ {len(warc_paths)} ä¸ªæ–‡ä»¶åˆ° {RAW_DIR} ...\")\n    for relative_path in warc_paths:\n        full_url = f\"{BASE_URL}/{relative_path}\"\n        download_file(full_url, RAW_DIR)\n    print(\"\\nğŸ‰ æ•°æ®å‡†å¤‡é˜¶æ®µå®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 3. æå– WARC æ­£æ–‡ =================\nfrom warcio.archiveiterator import ArchiveIterator\nimport trafilatura\n\nLIMIT_RECORDS = 10000 \nOUTPUT_FILE_STEP2 = os.path.join(PROCESSED_DIR, \"extracted_data.jsonl\")\n\ndef extract_text_from_warc(warc_path, output_path, limit=None):\n    print(f\"ğŸš€ å¼€å§‹å¤„ç†: {warc_path}\")\n    counter, success_count = 0, 0\n    \n    with open(output_path, 'w', encoding='utf-8') as out_f:\n        with open(warc_path, 'rb') as stream:\n            for record in tqdm(ArchiveIterator(stream), desc=\"Processing Records\"):\n                if record.rec_type == 'response':\n                    content_type = record.http_headers.get_header('Content-Type')\n                    if not content_type or 'text/html' not in content_type:\n                        continue\n                    try:\n                        content = record.content_stream().read()\n                    except Exception:\n                        continue\n                        \n                    text = trafilatura.extract(\n                        content, include_comments=False, \n                        include_tables=False, no_fallback=False\n                    )\n                    \n                    if text and len(text.strip()) > 0:\n                        url = record.rec_headers.get_header('WARC-Target-URI')\n                        data = {\"url\": url, \"text\": text}\n                        out_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n                        success_count += 1\n                \n                counter += 1\n                if limit and counter >= limit: break\n    \n    print(f\"\\nâœ… å¤„ç†å®Œæˆï¼æ‰«æè®°å½•æ•°: {counter}, æˆåŠŸæå–æ•°: {success_count}\")\n\nfiles = [f for f in os.listdir(RAW_DIR) if f.endswith('.warc.gz')]\nif files:\n    input_warc_path = os.path.join(RAW_DIR, files[0])\n    extract_text_from_warc(input_warc_path, OUTPUT_FILE_STEP2, LIMIT_RECORDS)\nelse:\n    print(\"âŒ æœªåœ¨ raw ç›®å½•ä¸‹æ‰¾åˆ° warc.gz æ–‡ä»¶ï¼Œè¯·ç¡®è®¤ä¸Šä¸€æ­¥æ˜¯å¦æˆåŠŸã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 4. åŸºç¡€è§„åˆ™æ¸…æ´— =================\nINPUT_FILE_STEP3 = OUTPUT_FILE_STEP2\nOUTPUT_FILE_STEP3 = os.path.join(PROCESSED_DIR, \"clean_data.jsonl\")\n\ndef is_high_quality(text):\n    if len(text) < 100 or len(text) > 2_000_000: return False\n        \n    words = text.split()\n    if len(words) == 0: return False\n    mean_word_len = sum(len(w) for w in words) / len(words)\n    if mean_word_len > 15: return False\n\n    code_symbols = {'{', '}', '[', ']', '<', '>', '\\\\'}\n    symbol_count = sum(1 for char in text if char in code_symbols)\n    if symbol_count / len(text) > 0.1: return False\n\n    bad_phrases = [\"lorem ipsum\", \"javascript is disabled\", \"enable cookies\",\n                   \"403 forbidden\", \"404 not found\", \"access denied\", \"rights reserved\"]\n    \n    text_lower = text.lower()\n    for phrase in bad_phrases:\n        if phrase in text_lower and len(text) < 500:\n            return False\n    return True\n\nif os.path.exists(INPUT_FILE_STEP3):\n    stats = {\"total\": 0, \"kept\": 0, \"dropped\": 0}\n    with open(INPUT_FILE_STEP3, 'r', encoding='utf-8') as f_in, \\\n         open(OUTPUT_FILE_STEP3, 'w', encoding='utf-8') as f_out:\n        \n        for line in tqdm(f_in, desc=\"Cleaning Data\"):\n            stats[\"total\"] += 1\n            try:\n                item = json.loads(line)\n                if is_high_quality(item.get(\"text\", \"\")):\n                    f_out.write(line)\n                    stats[\"kept\"] += 1\n                else:\n                    stats[\"dropped\"] += 1\n            except json.JSONDecodeError: continue\n\n    print(f\"\\nâœ… æ¸…æ´—å®Œæˆï¼æ€»æ•°: {stats['total']}, ä¸¢å¼ƒ: {stats['dropped']}, ä¿ç•™: {stats['kept']}\")\nelse:\n    print(f\"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {INPUT_FILE_STEP3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 5. å…¨å±€ Ray LSH å»é‡ =================\nimport ray\nfrom datasketch import MinHash, MinHashLSH\n\nINPUT_FILE_STEP4 = OUTPUT_FILE_STEP3\nOUTPUT_FILE_STEP4 = os.path.join(PROCESSED_DIR, \"deduplicated_data.jsonl\")\n\nNUM_PERM = 128 \nTHRESHOLD = 0.8  \n\nray.init(ignore_reinit_error=True)\n\n@ray.remote\ndef process_batch(lines, batch_id):\n    results = []\n    for line in lines:\n        try:\n            item = json.loads(line)\n            m = MinHash(num_perm=NUM_PERM)\n            for w in item['text'].split():\n                m.update(w.encode('utf8'))\n            results.append((item['url'], m, item['text']))\n        except Exception:\n            continue\n    return results\n\nif os.path.exists(INPUT_FILE_STEP4):\n    print(\"ğŸš€ ç¬¬ä¸€é˜¶æ®µ: å¹¶è¡Œè®¡ç®— MinHash ç­¾å...\")\n    with open(INPUT_FILE_STEP4, 'r', encoding='utf-8') as f:\n        all_lines = f.readlines()\n    \n    batch_size = 1000\n    batches = [all_lines[i:i + batch_size] for i in range(0, len(all_lines), batch_size)]\n    futures = [process_batch.remote(batch, i) for i, batch in enumerate(batches)]\n    \n    processed_batches = ray.get(futures)\n    results = [item for batch in processed_batches for item in batch]\n\n    print(\"\\nğŸš€ ç¬¬äºŒé˜¶æ®µ: æ„å»º LSH ç´¢å¼•å¹¶å»é‡...\")\n    lsh = MinHashLSH(threshold=THRESHOLD, num_perm=NUM_PERM)\n    unique_records, duplicate_count = [], 0\n    \n    for url, minhash, text in tqdm(results, desc=\"LSH Deduplication\"):\n        if len(lsh.query(minhash)) > 0:\n            duplicate_count += 1\n        else:\n            lsh.insert(url, minhash)\n            unique_records.append({\"url\": url, \"text\": text})\n\n    print(f\"\\nâœ… å»é‡å®Œæˆï¼å‘ç°é‡å¤: {duplicate_count}, å‰©ä½™æœ‰æ•ˆ: {len(unique_records)}\")\n    \n    with open(OUTPUT_FILE_STEP4, 'w', encoding='utf-8') as f:\n        for item in unique_records:\n            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n    ray.shutdown()\nelse:\n    print(f\"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {INPUT_FILE_STEP4}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 6. ä½¿ç”¨ FastText åˆ†ç¦»è¯­è¨€ =================\nimport fasttext\n\nINPUT_FILE_STEP5 = OUTPUT_FILE_STEP4\nFASTTEXT_MODEL_PATH = os.path.join(MODEL_DIR, 'lid.176.ftz')\n\nif os.path.exists(FASTTEXT_MODEL_PATH) and os.path.exists(INPUT_FILE_STEP5):\n    print(f\"åŠ è½½è¯­è¨€æ¨¡å‹: {FASTTEXT_MODEL_PATH}\")\n    fasttext.FastText.eprint = lambda x: None\n    model = fasttext.load_model(FASTTEXT_MODEL_PATH)\n    \n    files_out = {\n        'en': open(os.path.join(PROCESSED_DIR, 'data_en.jsonl'), 'w', encoding='utf-8'),\n        'zh': open(os.path.join(PROCESSED_DIR, 'data_zh.jsonl'), 'w', encoding='utf-8'),\n        'others': open(os.path.join(PROCESSED_DIR, 'data_others.jsonl'), 'w', encoding='utf-8')\n    }\n\n    count = 0\n    with open(INPUT_FILE_STEP5, 'r', encoding='utf-8') as f:\n        for line in tqdm(f, desc=\"Splitting Languages\"):\n            try:\n                data = json.loads(line)\n                text = data.get('text', '').replace('\\n', ' ')\n                if not text: continue\n\n                predictions = model.predict(text, k=1) \n                lang = predictions[0][0].replace('__label__', '')\n                \n                if lang == 'en': files_out['en'].write(json.dumps(data, ensure_ascii=False) + '\\n')\n                elif lang == 'zh': files_out['zh'].write(json.dumps(data, ensure_ascii=False) + '\\n')\n                else:\n                    data['detected_lang'] = lang \n                    files_out['others'].write(json.dumps(data, ensure_ascii=False) + '\\n')\n                count += 1\n            except Exception as e:\n                pass\n\n    for f in files_out.values(): f.close()\n    print(\"âœ… å¤„ç†å®Œæˆï¼ç”Ÿæˆäº† data_en.jsonl, data_zh.jsonl å’Œ data_others.jsonl\")\nelse:\n    print(f\"âŒ æ‰¾ä¸åˆ° FastText æ¨¡å‹æ–‡ä»¶æˆ–è¾“å…¥æ–‡ä»¶ï¼Œè¯·ç¡®è®¤ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================= 7. åŸºäº KenLM çš„é«˜è´¨é‡æ–‡æœ¬è¿‡æ»¤ =================\nimport kenlm\n\nINPUT_FILE_STEP6 = os.path.join(PROCESSED_DIR, \"data_en.jsonl\")\nOUTPUT_FILE_STEP6 = os.path.join(PROCESSED_DIR, \"final_data.jsonl\")\nKENLM_MODEL_PATH = os.path.join(MODEL_DIR, \"en.arpa.bin\")\nPERPLEXITY_THRESHOLD = -6.0\n\nif os.path.exists(INPUT_FILE_STEP6) and os.path.exists(KENLM_MODEL_PATH):\n    print(f\"ğŸš€ åŠ è½½ KenLM æ¨¡å‹: {KENLM_MODEL_PATH} ...\")\n    lm_model = kenlm.Model(KENLM_MODEL_PATH)\n    \n    stats = {\"total\": 0, \"kept\": 0, \"dropped\": 0}\n    print(f\"ğŸ”„ å¼€å§‹è´¨é‡è¿‡æ»¤ (é˜ˆå€¼: {PERPLEXITY_THRESHOLD})...\")\n    \n    with open(INPUT_FILE_STEP6, 'r', encoding='utf-8') as f_in, \\\n         open(OUTPUT_FILE_STEP6, 'w', encoding='utf-8') as f_out:\n        \n        for line in tqdm(f_in, desc=\"KenLM Filtering\"):\n            stats[\"total\"] += 1\n            try:\n                item = json.loads(line)\n                text = item.get(\"text\", \"\")\n                words = text.split()\n                num_words = len(words)\n                \n                if num_words < 3:\n                    stats[\"dropped\"] += 1\n                    continue\n\n                normalized_score = lm_model.score(text) / num_words\n                \n                if normalized_score > PERPLEXITY_THRESHOLD:\n                    item[\"perplexity_score\"] = normalized_score\n                    f_out.write(json.dumps(item, ensure_ascii=False) + '\\n')\n                    stats[\"kept\"] += 1\n                else:\n                    stats[\"dropped\"] += 1\n            except Exception:\n                continue\n\n    print(\"\\nğŸ‰ å…¨éƒ¨æµç¨‹ç»“æŸï¼\")\n    print(f\"   è¾“å…¥æ€»æ•°: {stats['total']}, ä¿ç•™ (é«˜è´¨é‡): {stats['kept']}, ä¸¢å¼ƒ: {stats['dropped']}\")\nelse:\n    print(f\"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ•°æ®æˆ– KenLM æ¨¡å‹: {KENLM_MODEL_PATH}ã€‚\")"
      ]
    }
  ]
}