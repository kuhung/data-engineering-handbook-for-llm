import requests
import gzip
import os
from tqdm import tqdm

# ================= é…ç½®éƒ¨åˆ† =================
# 1. é€‰æ‹© Crawl ID (è¿™æ˜¯ 2023 å¹´ç¬¬ 50 å‘¨çš„æŠ“å–æ•°æ®)
CRAWL_ID = "CC-MAIN-2023-50" 

# 2. ä¸‹è½½æ•°é‡ (Minié¡¹ç›®å»ºè®® 1-2 ä¸ªï¼Œæ¯ä¸ªçº¦ 1GB)
NUM_FILES_TO_DOWNLOAD = 1

# 3. æ•°æ®ä¿å­˜ç›®å½•
OUTPUT_DIR = "data/raw"

# Common Crawl åŸºç¡€ URL
BASE_URL = "https://data.commoncrawl.org"
# ===========================================

def get_warc_file_paths(crawl_id, num_files):
    """
    è·å–æŒ‡å®š Crawl ID çš„ WARC æ–‡ä»¶ä¸‹è½½è·¯å¾„åˆ—è¡¨
    """
    # è·¯å¾„ç´¢å¼•æ–‡ä»¶åœ°å€
    paths_url = f"{BASE_URL}/crawl-data/{crawl_id}/warc.paths.gz"
    print(f"ğŸ“¡ æ­£åœ¨è·å–æ–‡ä»¶ç´¢å¼•: {paths_url} ...")
    
    try:
        # æµå¼ä¸‹è½½ç´¢å¼•æ–‡ä»¶
        response = requests.get(paths_url, stream=True, timeout=10)
        response.raise_for_status()
        
        paths = []
        # è§£å‹å¹¶è¯»å–å‰ num_files è¡Œ
        with gzip.open(response.raw, 'rt', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= num_files:
                    break
                paths.append(line.strip())
        return paths
    except Exception as e:
        print(f"âŒ è·å–ç´¢å¼•å¤±è´¥: {e}")
        return []

def download_file(url, output_dir):
    """
    ä¸‹è½½å•ä¸ªæ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
    """
    local_filename = url.split('/')[-1]
    local_path = os.path.join(output_dir, local_filename)
    
    if os.path.exists(local_path):
        print(f"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {local_filename}")
        return local_path

    print(f"â¬‡ï¸ å¼€å§‹ä¸‹è½½: {local_filename}")
    
    try:
        # stream=True ç¡®ä¿ä¸ä¼šä¸€æ¬¡æ€§æŠŠ 1GB è¯»å…¥å†…å­˜
        with requests.get(url, stream=True, timeout=30) as r:
            r.raise_for_status()
            total_size = int(r.headers.get('content-length', 0))
            
            with open(local_path, 'wb') as f, tqdm(
                desc=local_filename,
                total=total_size,
                unit='iB',
                unit_scale=True,
                unit_divisor=1024,
            ) as bar:
                for chunk in r.iter_content(chunk_size=8192):
                    size = f.write(chunk)
                    bar.update(size)
        print(f"âœ… ä¸‹è½½å®Œæˆ: {local_path}")
        return local_path
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ {url}: {e}")
        # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œåˆ é™¤æŸåçš„åŠæˆå“æ–‡ä»¶
        if os.path.exists(local_path):
            os.remove(local_path)
        return None

def main():
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    # 1. è·å–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    warc_paths = get_warc_file_paths(CRAWL_ID, NUM_FILES_TO_DOWNLOAD)
    
    if not warc_paths:
        print("æœªæ‰¾åˆ°æ–‡ä»¶è·¯å¾„ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– CRAWL_IDã€‚")
        return

    print(f"ğŸ¯ è®¡åˆ’ä¸‹è½½ {len(warc_paths)} ä¸ªæ–‡ä»¶åˆ° {OUTPUT_DIR} ...")

    # 2. å¾ªç¯ä¸‹è½½
    for relative_path in warc_paths:
        full_url = f"{BASE_URL}/{relative_path}"
        download_file(full_url, OUTPUT_DIR)
    
    print("\nğŸ‰ æ•°æ®å‡†å¤‡é˜¶æ®µå®Œæˆï¼")

if __name__ == "__main__":
    main()
