{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ–¼ï¸ LLaVA å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ•°æ®æ„é€ æµæ°´çº¿\n",
    "æœ¬ Notebook æ•´åˆäº†ä» **COCO æ•°æ®å¯¹é½** åˆ° **å¤šæ¨¡æ€å¤§æ¨¡å‹(Qwen-VL) è‡ªåŠ¨æ ‡æ³¨** çš„å®Œæ•´æµç¨‹ã€‚é€‚ç”¨äºæ„å»º LLaVAã€Qwen-VL ç­‰æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ç¯å¢ƒå‡†å¤‡\n",
    "å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python tqdm openai numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ç¬¬ä¸€é˜¶æ®µï¼šç²¾å‡†åæ ‡å¯¹é½ (COCO Format to LLaVA)\n",
    "å°†æ ‡å‡†çš„ COCO æ£€æµ‹æ¡† `[x, y, w, h]` è½¬æ¢ä¸º LLaVA æ ¼å¼çš„å½’ä¸€åŒ–åæ ‡ `[ymin, xmin, ymax, xmax]` (0-1000)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob, cv2\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "IMAGE_DIR = \"../data/images/\"\n",
    "ANNOTATION_FILE = \"../data/annotations/instances_val2017.json\"\n",
    "OUTPUT_ALIGN_FILE = \"../data/llava_instruct_aligned.json\"\n",
    "\n",
    "def convert_bbox(bbox, width, height):\n",
    "    x, y, w, h = bbox\n",
    "    xmin = int((x / width) * 1000)\n",
    "    ymin = int((y / height) * 1000)\n",
    "    xmax = int((x + w) / width * 1000)\n",
    "    ymax = int((y + h) / height * 1000)\n",
    "    return [max(0, min(1000, ymin)), max(0, min(1000, xmin)), \n",
    "            max(0, min(1000, ymax)), max(0, min(1000, xmax))]\n",
    "\n",
    "def load_coco():\n",
    "    with open(ANNOTATION_FILE, 'r') as f: coco = json.load(f)\n",
    "    img_to_anns = {}\n",
    "    cat_map = {cat['id']: cat['name'] for cat in coco['categories']}\n",
    "    for ann in coco['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in img_to_anns: img_to_anns[img_id] = []\n",
    "        img_to_anns[img_id].append({\"bbox\": ann['bbox'], \"label\": cat_map.get(ann['category_id'], \"object\")})\n",
    "    return img_to_anns\n",
    "\n",
    "if os.path.exists(ANNOTATION_FILE):\n",
    "    img_to_anns = load_coco()\n",
    "    dataset = []\n",
    "    image_paths = glob.glob(os.path.join(IMAGE_DIR, \"*.jpg\"))\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=\"å¯¹é½æ•°æ®\"):\n",
    "        fname = os.path.basename(img_path)\n",
    "        try: image_id = int(fname.split('.')[0])\n",
    "        except: continue\n",
    "        anns = img_to_anns.get(image_id, [])\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None: continue\n",
    "        h, w, _ = img.shape\n",
    "        for ann in anns[:3]: # æ¯å¼ å›¾å–3ä¸ªç‰©ä½“\n",
    "            box = convert_bbox(ann['bbox'], w, h)\n",
    "            dataset.append({\n",
    "                \"id\": f\"{image_id}_{ann['label']}\",\n",
    "                \"image\": fname,\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"human\", \"value\": f\"Where is the {ann['label']}? <image>\"},\n",
    "                    {\"from\": \"qwen\", \"value\": f\"The {ann['label']} is at {box}.\"}\n",
    "                ]\n",
    "            })\n",
    "    with open(OUTPUT_ALIGN_FILE, 'w') as f: json.dump(dataset, f, indent=2)\n",
    "    print(f\"å¯¹é½å®Œæˆï¼Œä¿å­˜è‡³ {OUTPUT_ALIGN_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ç¬¬äºŒé˜¶æ®µï¼šLLM å¢å¼ºæ ‡æ³¨ (Cognitive Data Generation)\n",
    "åˆ©ç”¨ Qwen2.5-VL ç­‰å¤§æ¨¡å‹ç”ŸæˆåŒ…å«æ¨ç†ã€ç»†èŠ‚æè¿°çš„å¯¹è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = \"sk-lrdpxzsnhsbckhjrzekbrtccomruhcwzyrlwbroqwojtwtsw\"\n",
    "client = OpenAI(api_key=API_KEY, base_url=\"https://api.siliconflow.cn/v1\")\n",
    "\n",
    "def encode_img(p): \n",
    "    with open(p, \"rb\") as f: return base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "def generate_llava_json(img_path):\n",
    "    b64 = encode_img(img_path)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen2.5-VL-72B-Instruct\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"ç›´æ¥è¿”å›JSONï¼ŒåŒ…å«ä¸¤è½®å¯¹è¯ï¼šæè¿°å›¾ç‰‡å’Œç»†èŠ‚æé—®ã€‚ä½¿ç”¨[ymin, xmin, ymax, xmax]æ ¼å¼ã€‚\"},\n",
    "                  {\"role\": \"user\", \"content\": [{\"type\":\"text\",\"text\":\"åˆ†æå›¾ç‰‡\"}, \n",
    "                                               {\"type\":\"image_url\",\"image_url\":{\"url\":f\"data:image/jpeg;base64,{b64}\"}}]}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ç¬¬ä¸‰é˜¶æ®µï¼šå¤šå›¾å¯¹æ¯”æ•°æ® (Interleaved Data)\n",
    "æ„é€ ä¸¤å¼ å›¾ç‰‡äº¤é”™è¾“å…¥çš„å¯¹è¯ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹å¯¹æ¯”å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_interleaved(img1, img2):\n",
    "    prompt = \"Compare these two images.\"\n",
    "    # è°ƒç”¨æ¨¡å‹å¹¶æ„é€  LLaVA Interleaved æ ¼å¼...\n",
    "    # ç»“æœç¤ºä¾‹å¦‚ä¸‹ï¼š\n",
    "    return {\n",
    "        \"id\": \"compare_001\",\n",
    "        \"image\": [os.path.basename(img1), os.path.basename(img2)],\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Image 1: <image>\\nImage 2: <image>\\nWhat is the difference?\"},\n",
    "            {\"from\": \"qwen\", \"value\": \"The first image is a city, the second is a park.\"}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ç¬¬å››é˜¶æ®µï¼šå¯è§†åŒ–éªŒè¯ (Debug Visualization)\n",
    "è¯»å–ç”Ÿæˆçš„ JSON æ–‡ä»¶ï¼Œå°†åæ ‡æ¡†åå‘ç”»åœ¨å›¾ç‰‡ä¸Šï¼Œæ£€æŸ¥ LLM è¾“å‡ºçš„åæ ‡æ˜¯å¦ç²¾å‡†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "VIZ_OUT = \"../data/viz_debug/\"\n",
    "os.makedirs(VIZ_OUT, exist_ok=True)\n",
    "\n",
    "with open(OUTPUT_ALIGN_FILE, 'r') as f: data = json.load(f)\n",
    "\n",
    "for entry in data[:5]: # æŠ½æ ·5å¼ éªŒè¯\n",
    "    img = cv2.imread(os.path.join(IMAGE_DIR, entry['image']))\n",
    "    h, w, _ = img.shape\n",
    "    for turn in entry['conversations']:\n",
    "        bboxes = re.findall(r'\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]', turn['value'])\n",
    "        for b in bboxes:\n",
    "            ymin, xmin, ymax, xmax = [int(x) for x in b]\n",
    "            cv2.rectangle(img, (int(xmin*w/1000), int(ymin*h/1000)), \n",
    "                          (int(xmax*w/1000), int(ymax*h/1000)), (0, 255, 0), 2)\n",
    "    cv2.imwrite(os.path.join(VIZ_OUT, f\"viz_{entry['image']}\"), img)\n",
    "print(f\"éªŒè¯å›¾ç‰‡å·²ä¿å­˜è‡³ {VIZ_OUT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
